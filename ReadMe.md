# ğŸš€ **vLLM.rs** â€“ A Minimalist vLLM in Rust

A blazing-fast âš¡, lightweight **Rust** ğŸ¦€ implementation of vLLM.

---

## âœ¨ Key Features

* ğŸ”§ **Pure Rust** â€“ Absolutely **no** PyTorch or Python required
* ğŸš€ **High Performance** â€“ On par with the original vLLM (PyTorch + ATen)
* ğŸ§  **Minimalist Core** â€“ Core logic in **< 1000 lines** of clean Rust code
* ğŸ’» **Cross-Platform** â€“ Works on both **CUDA** (Linux/Windows) and **Metal** (macOS)
* ğŸ¤– **Built-in Chatbot** â€“ Built-in Rust Chatbot work with **CUDA** and **Metal**
* ğŸ¤ **Open for Contributions** â€“ PRs, issues, and stars are welcome!

---

## ğŸ“¦ Usage

Make sure you have the [Rust toolchain](https://www.rust-lang.org/tools/install) installed.

Mac OS Platform (Metal) requires installation of [XCode command line tools](https://mac.install.guide/commandlinetools/).

---

### ğŸ”¥ CUDA (Linux/Windows) and ğŸ Metal (macOS)

âš ï¸ First run may take a while on CUDA (if flash attention enabled).

---

### ğŸ¤–âœ¨ Interactive Mode

Simply run the program with `--i` parameter:

```bash
# ğŸ”¥ CUDA
cargo run --release --features cuda -- --i --w /path/DeepSeek-R1-Distill-Llama-8B-Q2_K.gguf

# ğŸ”¥ CUDA with ğŸš€ Flash Attention (build takes longer time)
cargo run --release --features cuda,flash-attn -- --i --w /path/DeepSeek-R1-Distill-Llama-8B-Q2_K.gguf

# ğŸ Metal (macOS)
cargo run --release --features metal -- --i --w /path/DeepSeek-R1-Distill-Llama-8B-Q2_K.gguf

```


### ğŸ“½ï¸ Demo Video

Watch a quick demo of how it works! ğŸ‰

<video src="https://github.com/user-attachments/assets/0751471b-a0c4-45d7-acc6-99a3e91e4c91" width="70%"></video>


### ğŸ§¾âœ¨ Completion Mode

#### GGUF model:

```bash
# ğŸ”¥ CUDA
cargo run --release --features cuda -- --w /path/qwq-32b-q4_k_m.gguf --prompts "How are you today?"

# ğŸ”¥ CUDA with ğŸš€ Flash Attention (build takes longer time)
cargo run --release --features cuda,flash-attn -- --w /path/qwq-32b-q4_k_m.gguf --prompts "How are you today?"

# ğŸ Metal (macOS)
cargo run --release --features cuda -- --w /path/qwq-32b-q4_k_m.gguf --prompts "How are you today?"
```

#### Safetensor model:

```bash

# ğŸ”¥ CUDA
cargo run --release --features cuda -- --w /path/Qwen3-8B/ --prompts "How are you today?"

# ğŸ Metal (macOS)
cargo run --release --features metal -- --w /path/Qwen3-8B/ --prompts "How are you today?"

```

---

### ğŸ“š Batched Requests

Prompts are separated by `|`

```bash
# GGUF model
cargo run --release --features cuda -- --w /path/qwq-32b-q4_k_m.gguf --prompts "Please talk about China. | Please talk about America."

# Safetensor model
cargo run --release --features metal -- --w /path/Qwen3-8B/ --prompts "Please talk about China. | Please talk about America."
```

---

### ğŸ—œï¸ In-situ Quantization (GGUF format conversion)

Takes a few minutes for quantization.

```bash
# macOS
cargo run --release --features metal -- --w /path/Qwen3-0.6B/ --quant q4k --prompts "How are you today?"

# CUDA
cargo run --release --features cuda -- --w /path/Qwen3-8B/ --quant q4k --prompts "How are you today?"
```

---

## ğŸ“„ Sample Output

**Single request** with Qwen3-0.6B (BF16) on macOS/Metal:

```bash
cargo run --features metal -- --w /path/Qwen3-0.6B/ --prompts "How are you today?"
```

```
<think>
Okay, the user asked, "How are you today?"...
</think>

Hi there! How are you today? I'm here to help you with anything! ğŸ˜Š Let me know if there's anything you need!
```

---

### ğŸ“Š Batched Results (Examples)

**LLaMa3.1-8B** BF16 (16 requests on A100):

```bash
8450 tokens generated in 14.28 s (decoding throughput: 591.82 tokens/s)
```

**QwQ-32B** GGUF Q4K (4 requests on A100):

```
4000 tokens in 48.23s (avg throughput: 82.93 tokens/s)
```

---

## âš™ï¸ Command-Line Parameters

| Flag        | Description                                       |    |
| ----------- | ------------------------------------------------- | -- |
| `--w`       | Path to model folder (Safetensor) or file (GGUF)  |    |
| `--d`       | Device ID (e.g. `--d "0"`)                        |    |
| `--kvmem`   | KV cache size in MB (default: `4096`)               |    |
| `--max`   | Maximum number of tokens in each chat response (default: `4096`, up to `max_model_len`) |    |
| `--prompts` | Input prompts separated by "\|" |
| `--dtype`   | KV cache dtype: `bf16` (default), `f16`, or `f32` |    |

---

## ğŸ§  Supported Architectures

* âœ… LLaMa (LLaMa2, LLaMa3)
* âœ… Qwen (Qwen2, Qwen3)
* âœ… Mistral

Supports both **Safetensor** and **GGUF** formats.

---

## ğŸ“Œ Status

> **Project is under active development. Expect changes.**

---

## ğŸ› ï¸ TODO

* [x] ğŸ”§ Fix batched inference on `Metal`
* [ ] ğŸ›°ï¸ Multi-rank inference
* [ ] ğŸ§  More model support
* [x] ğŸ§¾ GGUF support
* [ ] ğŸŒ OpenAI-compatible API server (w/ streaming)
* [x] âš¡ FlashAttention (CUDA)
* [ ] â™»ï¸ Continuous batching

---

## ğŸ“š Reference

Core ideas inspired by:

* [Candle-vLLM](https://github.com/EricLBuehler/candle-vllm)
* Python nano-vllm

---

ğŸ’¡ **Like the project? Star it â­ and contribute!**

---
