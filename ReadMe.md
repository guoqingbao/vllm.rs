# ğŸš€ **vLLM.rs** â€“ A Minimalist vLLM in Rust

A blazing-fast âš¡, lightweight **Rust** ğŸ¦€ implementation of vLLM.

---

## âœ¨ Key Features

* ğŸ”§ **Pure Rust** â€“ Absolutely **no** PyTorch required
* ğŸš€ **High Performance** â€“ On par with the original vLLM (PyTorch + ATen)
* ğŸ§  **Minimalist Core** â€“ Core logic in **< 1000 lines** of clean Rust code
* ğŸ’» **Cross-Platform** â€“ Works on both **CUDA** (Linux/Windows) and **Metal** (macOS)
* ğŸ¤– **Built-in Chatbot** â€“ Built-in Rust Chatbot work with **CUDA** and **Metal**
* ğŸ¤– **Python PyO3 interface** â€“ Lightweight Python interface for chat completion
* ğŸ¤ **Open for Contributions** â€“ PRs, issues, and stars are welcome!

---

## ğŸ“¦ Usage

Make sure you have the [Rust toolchain](https://www.rust-lang.org/tools/install) installed.

Mac OS Platform (Metal) requires installation of [XCode command line tools](https://mac.install.guide/commandlinetools/).

Python package build requires [Maturin](https://github.com/PyO3/maturin/).

**Quick Usage:**

```python
cfg = EngineConfig(model_path = "/path/Qwen3-8B-Q2_K.gguf", ...)
engine = Engine(cfg, "bf16")
params = SamplingParams(temperature = 0.6, max_tokens = 256)
prompt = engine.apply_chat_template([Message("user", "How are you?")], True)
outputs = engine.generate(params, [prompt])
print(outputs)
```
---

### ğŸ”¥ CUDA (Linux/Windows) and ğŸ Metal (macOS)

âš ï¸ First run may take a while on CUDA (if flash attention enabled).

---

### ğŸ¤–âœ¨ Interactive Mode (Pure Rust)

Simply run the program with `--i` and `--w` parameter:

```bash
# ğŸ”¥ CUDA (for short context)
cargo run --release --features cuda -- --i --w /path/DeepSeek-R1-Distill-Llama-8B-Q2_K.gguf

# ğŸ”¥ CUDA with âš¡ Flash Attention (for extra-long context, e.g., 32k inputs, but build takes longer time)
cargo run --release --features cuda,flash-attn -- --i --w /path/DeepSeek-R1-Distill-Llama-8B-Q2_K.gguf

# ğŸ Metal (macOS)
cargo run --release --features metal -- --i --w /path/DeepSeek-R1-Distill-Llama-8B-Q2_K.gguf

```

### ğŸ¤–âœ¨ Interactive Mode (Python Interface)

Install Maturin and build Python package

```bash
pip install maturin
pip install maturin[patchelf] #Linux/Windows
```

Use `-i` in Maturin build for seleting Python version, e.g., `-i 3.9`

```bash
# ğŸ”¥ CUDA (for short context)
maturin build --release --features cuda,python

# ğŸ”¥ CUDA with âš¡ Flash Attention (for extra-long context, e.g., 32k inputs, but build takes longer time)
maturin build --release --features cuda,flash-attn,python

# ğŸ Metal (macOS)
maturin build --release --features metal,python
```

Install Python package and run the demo

```bash
python3 -m pip install target/wheels/vllm_rs-0.1.0*.whl
python3 example/chat.py --i --w /path/DeepSeek-R1-Distill-Llama-8B-Q2_K.gguf
python3 example/chat.py --w /path/DeepSeek-R1-Distill-Llama-8B-Q2_K.gguf --prompts "How are you? | Who are you?"
```


### ğŸ“½ï¸ Demo Video

Watch a quick demo of how it works! ğŸ‰

<video src="https://github.com/user-attachments/assets/0751471b-a0c4-45d7-acc6-99a3e91e4c91" width="70%"></video>


### ğŸ§¾âœ¨ Completion Mode

#### GGUF model:

```bash
# ğŸ”¥ CUDA (for short context)
cargo run --release --features cuda -- --w /path/qwq-32b-q4_k_m.gguf --prompts "How are you today?"

# ğŸ”¥ CUDA with âš¡ Flash Attention (for extra-long context, e.g., 32k inputs, but build takes longer time)
cargo run --release --features cuda,flash-attn -- --w /path/qwq-32b-q4_k_m.gguf --prompts "How are you today?"

# ğŸ Metal (macOS)
cargo run --release --features cuda -- --w /path/qwq-32b-q4_k_m.gguf --prompts "How are you today?"
```

#### Safetensor model:

```bash

# ğŸ”¥ CUDA
cargo run --release --features cuda,flash-attn -- --w /path/Qwen3-8B/ --prompts "How are you today?"

# ğŸ Metal (macOS)
cargo run --release --features metal -- --w /path/Qwen3-8B/ --prompts "How are you today?"

```

---

### ğŸ“š Batched Requests

Prompts are separated by `|`

```bash
# GGUF model
cargo run --release --features cuda,flash-attn -- --w /path/qwq-32b-q4_k_m.gguf --prompts "Please talk about China. | Please talk about America."

# Safetensor model
cargo run --release --features metal -- --w /path/Qwen3-8B/ --prompts "Please talk about China. | Please talk about America."
```

---

### ğŸ—œï¸ In-situ Quantization (GGUF format conversion)

Takes a few minutes for quantization.

```bash
# macOS
cargo run --release --features metal -- --w /path/Qwen3-0.6B/ --quant q4k --prompts "How are you today?"

# CUDA
cargo run --release --features cuda,flash-attn -- --w /path/Qwen3-8B/ --quant q4k --prompts "How are you today?"
```

---

## ğŸ“„ Sample Output

**Single request** with Qwen3-0.6B (BF16) on macOS/Metal:

```bash
cargo run --features metal -- --w /path/Qwen3-0.6B/ --prompts "How are you today?"
```

```
<think>
Okay, the user asked, "How are you today?"...
</think>

Hi there! How are you today? I'm here to help you with anything! ğŸ˜Š Let me know if there's anything you need!
```

---

### ğŸ“Š Batched Results (Examples)

**LLaMa3.1-8B** BF16 (16 requests on A100):

```bash
8450 tokens generated in 14.28 s (decoding throughput: 591.82 tokens/s)
```

**QwQ-32B** GGUF Q4K (4 requests on A100):

```
4000 tokens in 48.23s (avg throughput: 82.93 tokens/s)
```

---

## âš™ï¸ Command-Line Parameters

| Flag        | Description                                       |    |
| ----------- | ------------------------------------------------- | -- |
| `--w`       | Path to model folder (Safetensor) or file (GGUF)  |    |
| `--d`       | Device ID (e.g. `--d "0"`)                        |    |
| `--kvmem`   | KV cache size in MB (default: `4096`)               |    |
| `--max`   | Maximum number of tokens in each chat response (default: `4096`, up to `max_model_len`) |    |
| `--prompts` | Input prompts separated by "\|" |
| `--dtype`   | KV cache dtype: `bf16` (default), `f16`, or `f32` |    |

---

## ğŸ§  Supported Architectures

* âœ… LLaMa (LLaMa2, LLaMa3)
* âœ… Qwen (Qwen2, Qwen3)
* âœ… Mistral

Supports both **Safetensor** and **GGUF** formats.

---

## ğŸ“Œ Status

> **Project is under active development. Expect changes.**

---

## ğŸ› ï¸ TODO

* [x] ğŸ”§ Fix batched inference on `Metal`
* [ ] ğŸ›°ï¸ Multi-rank inference
* [ ] ğŸ§  More model support
* [x] ğŸ§¾ GGUF support
* [ ] ğŸŒ OpenAI-compatible API server (w/ streaming)
* [x] âš¡ FlashAttention (CUDA)
* [ ] â™»ï¸ Continuous batching

---

## ğŸ“š Reference

Core ideas inspired by:

* [Candle-vLLM](https://github.com/EricLBuehler/candle-vllm)
* Python nano-vllm

---

ğŸ’¡ **Like the project? Star it â­ and contribute!**

---
