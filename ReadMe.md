# ğŸš€ **vLLM.rs** â€“ A Minimalist vLLM in Rust

A blazing-fast âš¡, lightweight **Rust** ğŸ¦€ implementation of vLLM.

---

<p align="center">
  <a href="./ReadMe.md">English</a> |
  <a href="./ReadMe-CN.md">ç®€ä½“ä¸­æ–‡</a> |
</p>

## âœ¨ Key Features

* ğŸ”§ **Pure Rust Backend** â€“ Absolutely **no** PyTorch required
* ğŸš€ **High Performance** â€“ Comparable to original vLLM (PyTorch + ATen)
* ğŸ§  **Minimalist Core** â€“ Core logic written in **< 1000 lines** of clean Rust
* ğŸ’» **Cross-Platform** â€“ Supports **CUDA** (Linux/Windows) and **Metal** (macOS)
* ğŸ¤– **Built-in Chatbot/API Server** â€“ Native Rust server for both CUDA and Metal
* ğŸ **Lightweight Python Interface** â€“ PyO3-powered bindings for chat completion
* ğŸ¤ **Open for Contributions** â€“ PRs, issues, and stars are welcome!

---

## ğŸ“¦ Installation & Usage

> âš ï¸ The first build may take time if Flash Attention is enabled.

### ğŸ› ï¸ Prerequisites

* Install the [Rust toolchain](https://www.rust-lang.org/tools/install)
* On macOS, install [Xcode command line tools](https://mac.install.guide/commandlinetools/)
* For Python bindings, install [Maturin](https://github.com/PyO3/maturin)

---

## ğŸ Quick Python Example

```python
cfg = EngineConfig(model_path="/path/Qwen3-8B-Q2_K.gguf", ...)
engine = Engine(cfg, "bf16")
params = SamplingParams(temperature=0.6, max_tokens=256)
prompt = engine.apply_chat_template([Message("user", "How are you?")], True)

# Synchronous generation for batched input
outputs = engine.generate_sync(params, [prompt, prompt])
print(outputs)

# Streaming generation for single request
stream = engine.generate_stream(params, prompt)
for token in stream:
    print(token)
```

---

## ğŸ¤–âœ¨ Interactive Mode (Rust CLI)

Run with `--i` for interactive chat and `--w` to specify model path:

```bash
# CUDA (short context)
cargo run --release --features cuda -- --i --w /path/qwq-32b-q4_k_m.gguf

# CUDA with Flash Attention (long context, e.g., 32k tokens)
cargo run --release --features cuda,flash-attn -- --i --w /path/qwq-32b-q4_k_m.gguf

# macOS (Metal)
cargo run --release --features metal -- --i --w /path/DeepSeek-R1-Distill-Llama-8B-Q2_K.gguf
```

---

## ğŸŒâœ¨ API Server Mode (Python Interface)

1. **Install Maturin**

```bash
pip install maturin
pip install maturin[patchelf]  # For Linux/Windows
```

2. **Build the Python package**

   ğŸ’¡ Specify Python version with `-i`, e.g., `-i python3.9`

```bash
# CUDA (short context)
maturin build --release --features cuda,python

# CUDA with Flash Attention
maturin build --release --features cuda,flash-attn,python -i 3.9

# macOS (Metal)
maturin build --release --features metal,python
```

3. **Install and Setup Chat Server**

```bash
pip install target/wheels/vllm_rs-0.1.0*.whl
pip install fastapi uvicorn
```

4. **Start OpenAI API Server**

```bash
python example/server.py --w /path/qwq-32b-q4_k_m.gguf --host 0.0.0.0 --port 8000
```
ğŸ’¡ You can use any client compatible with the OpenAI API.

### Other Examples:

```bash
# Interactive chat
python3 example/chat.py --i --w /path/qwq-32b-q4_k_m.gguf

# Chat completion
python3 example/completion.py --w /path/qwq-32b-q4_k_m.gguf --prompts "How are you? | How to make money?"
```

---

### ğŸ“½ï¸ Demo Video

Watch it in action ğŸ‰ <video src="https://github.com/user-attachments/assets/0751471b-a0c4-45d7-acc6-99a3e91e4c91" width="70%"></video>

---

## ğŸ§¾ Completion Mode (Rust CLI)

### GGUF Models

```bash
# CUDA
cargo run --release --features cuda -- --w /path/qwq-32b-q4_k_m.gguf --prompts "How are you today?"

# CUDA + Flash Attention
cargo run --release --features cuda,flash-attn -- --w /path/qwq-32b-q4_k_m.gguf --prompts "How are you today?"

# Metal (macOS)
cargo run --release --features metal -- --w /path/qwq-32b-q4_k_m.gguf --prompts "How are you today?"
```

### With Python:

```bash
python example/completion.py --w /path/qwq-32b-q4_k_m.gguf --prompts "How are you? | How to make money?"
```

### Safetensor Models (Unquantized)

```bash
# CUDA
cargo run --release --features cuda,flash-attn -- --w /path/Qwen3-8B/ --prompts "How are you today?"

# Metal
cargo run --release --features metal -- --w /path/Qwen3-8B/ --prompts "How are you today?"
```

---

## ğŸ“š Batched Requests

Use `|` to separate prompts:

```bash
# GGUF (Rust)
cargo run --release --features cuda,flash-attn -- --w /path/qwq-32b-q4_k_m.gguf --prompts "Talk about China. | Talk about America."

# Safetensor (Rust)
cargo run --release --features metal -- --w /path/Qwen3-8B/ --prompts "Talk about China. | Talk about America."

# GGUF (Python)
python3 example/completion.py --w /path/qwq-32b-q4_k_m.gguf --prompts "How are you? | How to make money?"
```

---

## ğŸ—œï¸ In-Situ Quantization (GGUF Conversion)

This may take several minutes:

```bash
# macOS
cargo run --release --features metal -- --w /path/Qwen3-0.6B/ --quant q4k --prompts "How are you today?"

# CUDA
cargo run --release --features cuda,flash-attn -- --w /path/Qwen3-8B/ --quant q4k --prompts "How are you today?"
```

---

## ğŸ“„ Sample Output

**Single request** (Qwen3-0.6B, BF16, macOS Metal):

```bash
cargo run --features metal -- --w /path/Qwen3-0.6B/ --prompts "How are you today?"
```

```
<think>
Okay, the user asked, "How are you today?"...
</think>

Hi there! How are you today? I'm here to help you with anything! ğŸ˜Š Let me know if there's anything you need!
```

---

## ğŸ“Š Batched Output Examples

**LLaMa3.1-8B (BF16, A100, 16 requests)**

```
8450 tokens generated in 14.28s (591.82 tokens/s)
```

**QwQ-32B GGUF Q4K (A100, 4 requests)**

```
4000 tokens in 48.23s (82.93 tokens/s)
```

---

## âš™ï¸ CLI Flags

| Flag        | Description                                                      |    |
| ----------- | ---------------------------------------------------------------- | -- |
| `--w`       | Path to model folder (Safetensor) or file (GGUF)                 |    |
| `--d`       | Device ID (e.g. `--d 0`)                                         |    |
| `--kvmem`   | KV cache size in MB (default: `4096`)                            |    |
| `--max`     | Max tokens per response (default: `4096`, up to `max_model_len`) |    |
| `--prompts` | Prompts, separated by \`                                         | \` |
| `--dtype`   | KV cache dtype: `bf16` (default), `f16`, or `f32`                |    |

---

## ğŸ§  Supported Architectures

* âœ… LLaMa (LLaMa2, LLaMa3)
* âœ… Qwen (Qwen2, Qwen3)
* âœ… Mistral

Supports both **Safetensor** and **GGUF** formats.

---

## ğŸ“Œ Project Status

> ğŸš§ **Under active development â€“ breaking changes may occur!**

---

## ğŸ› ï¸ Roadmap

* [x] Batched inference (Metal)
* [x] GGUF format support
* [x] FlashAttention (CUDA)
* [x] OpenAI-compatible API (streaming support)
* [x] Continuous batching
* [ ] Multi-rank inference
* [ ] Additional model support

---

## ğŸ“š References

Inspired by:

* [Candle-vLLM](https://github.com/EricLBuehler/candle-vllm)
* Python nano-vllm

---

ğŸ’¡ **Like this project? Give it a â­ and contribute!**
