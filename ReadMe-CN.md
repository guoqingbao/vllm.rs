# ğŸš€ **vLLM.rs** â€“ ç”¨ Rust å®ç°çš„æç®€ vLLM

ä¸€ä¸ªæé€Ÿ âš¡ã€è½»é‡çš„ ğŸ¦€**Rust å®ç°ç‰ˆ vLLM**ã€‚

---

<p align="center">
  <a href="./ReadMe.md">English</a> |
  <a href="./ReadMe-CN.md">ç®€ä½“ä¸­æ–‡</a> |
</p>

## âœ¨ ä¸»è¦ç‰¹æ€§

* ğŸ”§ **çº¯ Rust åç«¯** â€“ å®Œå…¨**ä¸ä¾èµ– PyTorch**
* ğŸš€ **é«˜æ€§èƒ½** (æ”¯æŒ**ä¸Šä¸‹æ–‡ç¼“å­˜ã€PDåˆ†ç¦»**) â€“ æ€§èƒ½ä¼˜äºPythonåŒç±»æ¨ç†æ¡†æ¶
* ğŸ§  **æç®€æ ¸å¿ƒ** â€“ æ ¸å¿ƒé€»è¾‘ä»… **~ 3000 è¡Œ** Rust ä»£ç 
* ğŸ’» **è·¨å¹³å°æ”¯æŒ** â€“ æ”¯æŒ **CUDA**ï¼ˆLinux/Windowsï¼‰ä¸ **Metal**ï¼ˆmacOSï¼‰
* ğŸ¤– **å†…ç½®èŠå¤©/API æœåŠ¡** â€“ Rust åŸç”Ÿå®ç°çš„èŠå¤©ä¸ API æœåŠ¡
* ğŸ **è½»é‡ Python æ¥å£** â€“ ä½¿ç”¨ PyO3 æ„å»ºçš„ Python èŠå¤©æ¥å£
* ğŸ¤ **æ¬¢è¿è´¡çŒ®** â€“ æ¬¢è¿æäº¤ PRã€é—®é¢˜æˆ–ç»™é¡¹ç›®ç‚¹äº® â­ï¼

---
### å¯¹è¯æ€§èƒ½

> **A100** (å•å¡, 40G)

| æ¨¡å‹ | æ ¼å¼ | å¤§å° | è¾“å‡ºé€Ÿåº¦ |
|------------------|---------------|----------|------------------------|
| Llama-3.1-8B | ISQ (BF16->Q4K) | 8B | **90.19** tokens/s |
| DeepSeek-R1-Distill-Llama-8B | Q2_K | 8B | **94.47** tokens/s |
| DeepSeek-R1-0528-Qwen3-8B | Q4_K_M | 8B | **95** tokens/s |
| GLM-4-9B-0414 | Q4_K_M | 9B | **70.38** tokens/s |
| QwQ-32B | Q4_K_M | 32B | **35.69** tokens/s |
| **Qwen3-30B-A3B** | Q4_K_M | **30B (MoE)** | **75.91** tokens/s  |

#### vLLM.rs åœ¨ **Metal (Apple Silicon, M4)** ä¸Šçš„æ€§èƒ½
> æ¨¡å‹: Qwen3-0.6B (BF16), Qwen3-4B (Q4_K_M), Qwen3-8B (Q2_K)ï¼›
> å¹¶å‘è¯·æ±‚æ•°: 1 - 128ï¼›
> Max Model Length: 512 - 2048ï¼›
> æ¯ä¸ªè¯·æ±‚æœ€å¤§è¾“å‡º: 512 - 2048ï¼›

| æ¨¡å‹ | å¹¶å‘æ•° | è¾“å‡ºTokens | è€—æ—¶ (s) | ååé‡ (tokens/s) |
|------------------|--------|--------|---------|-------------|
| Qwen3-0.6B (BF16) |  128  | 63488       | 83.13s    | 763.73     |
| Qwen3-0.6B (BF16) |  32      | 15872       | 23.53s    | 674.43    |
| Qwen3-0.6B (BF16) | 1       | 456       | 9.23s    | 49.42       |
| Qwen3-4B (Q4_K_M)  | 1       | 1683       | 52.62s    | 31.98     |
| Qwen3-8B (Q2_K)  | 1       | 1300       | 80.88s    | 16.07     |

### æ€§èƒ½å¯¹æ¯”

> æ¨¡å‹: Qwen3-0.6B (BF16)ï¼›
> å¹¶å‘è¯·æ±‚æ•°: 256ï¼›
> Max Model Length: 1024ï¼›
> æ¯ä¸ªè¯·æ±‚æœ€å¤§è¾“å‡º: 1024

| æ¨ç†å¼•æ“ | Tokens | è€—æ—¶ (s) | ååç‡ (tokens/s) |
|------------------|---------------|----------|------------------------|
| vLLM (RTX 4070) (Reference)          | 133,966       | 98.37    | 1361.84                |
| Nano-vLLM (RTX 4070) (Reference)      | 133,966       | 93.41    | 1434.13                |
| **vLLM.rs** (**A100**)        | 262,144       | 23.88s    | **10977.55** (**æå‡40%+**)               |
| Nano-vLLM (A100)       | 262,144       | 34.22s    |   7660.26      | 

<a href="python/ReadMe.md">å¤ç°æ­¥éª¤</a>


## ğŸ§  æ”¯æŒçš„æ¨¡å‹æ¶æ„

* âœ… LLaMa ç³»åˆ—ï¼ˆLLaMa2ã€LLaMa3ï¼‰
* âœ… Qwen ç³»åˆ—ï¼ˆQwen2ã€Qwen3ï¼‰
* âœ… Qwen2 Moe ç³»åˆ—ï¼ˆä½¿ç”¨Qwen3 MoEæµç¨‹+å…±äº«ä¸“å®¶å±‚ï¼‰
* âœ… Qwen3 MoE ç³»åˆ—
* âœ… Mistral
* âœ… GLM4 (0414ç‰ˆæœ¬, **éChatGLM**)

æ”¯æŒ **Safetensor** (åŒ…å«GPTQ, AWQé‡åŒ–æ ¼å¼) å’Œ **GGUF** æ ¼å¼ã€‚

## ğŸ“½ï¸ æ¼”ç¤ºè§†é¢‘

ğŸ‰ è§‚çœ‹é¡¹ç›®è¿è¡Œæ¼”ç¤ºï¼š
<video src="https://github.com/user-attachments/assets/7fc6aa0b-78ac-4323-923f-d761dd12857f" width="1000px"></video>


## ğŸ“˜ ä½¿ç”¨æ–¹æ³•ï¼ˆRustï¼‰

ä½¿ç”¨ `--i` å¯ç”¨äº¤äº’æ¨¡å¼ ğŸ¤–ï¼Œ`--server` å¯ç”¨æœåŠ¡æ¨¡å¼ ğŸŒï¼Œ`--m`æŒ‡å®šHuggingfaceæ¨¡å‹ï¼Œæˆ–`--w` æŒ‡å®šæœ¬åœ°Safetensorsæ¨¡å‹è·¯å¾„ æˆ–`--f` æŒ‡å®šGGUFæ¨¡å‹æ–‡ä»¶ï¼š

```bash
# å•å¡æ¨ç† CUDA + Built-in Context Cache (ä½¿ç”¨ `--fp8-kvcache` å¯ç”¨ FP8 KV Cache)
cargo run --release --features cuda,nccl -- --i --d 0 --m unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF --f Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --max-model-len 262144 --context-cache

# å¤šå¡æ¨ç† CUDA Graph + Flash Attentionï¼ˆä½¿ç”¨run.shç”Ÿæˆç‹¬ç«‹runnerï¼‰
./run.sh --release --features cuda,nccl,graph,flash-attn -- --i --d 0,1 --f /path/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --max-model-len 262144 --context-cache

# å¤šå¡æ¨ç† server æœåŠ¡ (æœªé‡åŒ–æ¨¡å‹)
./run.sh --release --features cuda,nccl,flash-attn -- --d 0,1,2,3 --w /path/Qwen3-30B-A3B-Instruct-2507 --max-model-len 100000 --max-num-seqs 4 --server --port 8000

# å¤šå¡æ¨ç† server æœåŠ¡ (å¯é€‰`--fp8-kvcache` æˆ– `--context-cache`)
./run.sh --release --features cuda,nccl,flash-attn -- --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --max-model-len 100000 --max-num-seqs 4 --server --port 8000 --fp8-kvcache

# å¤šå¡æ¨ç† server æœåŠ¡ (é‡åŒ–å¹¶åŠ è½½ä¸ºQ4Kæ ¼å¼ï¼Œå¯é€‰`--context-cache`ï¼ŒåŒæ—¶ä½¿ç”¨Flash Attentionåšdecoding)
./run.sh --release --features cuda,nccl,flash-context -- --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --max-model-len 100000 --max-num-seqs 4 --server --port 8000 --context-cache

# CUDA Graphå’Œè¾“å‡ºæƒ©ç½šé¡¹
cargo run --release --features cuda,graph -- --i --f /path/qwq-32b-q4_k_m.gguf --presence-penalty 1.2 --frequency-penalty 1.2

# macOSï¼ˆMetalï¼‰
cargo run --release --features metal -- --i --f /path/DeepSeek-R1-Distill-Llama-8B-Q2_K.gguf

#macOS (Metal, ISQ) with context cache
cargo run --release --features metal -- --i --w /path/Qwen3-0.6B --isq q6k --context-cache
```

## Prefill-decode åˆ†ç¦»ï¼ˆPDåˆ†ç¦»ï¼‰
```shell
# å¯åŠ¨PDæœåŠ¡å™¨ (æ— éœ€æŒ‡å®š`port`ï¼Œå› ä¸ºæ­¤æœåŠ¡å™¨ä¸ç›´æ¥æ¥æ”¶ç”¨æˆ·è¯·æ±‚)
# Rust
./run.sh --release --features cuda,nccl,flash-attn -- --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --max-model-len 200000 --max-num-seqs 2 --server --pd-server
# Python (ä¾èµ–ï¼špip install vllm_rs fastapi uvicorn)
python3 -m vllm_rs.server --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --max-model-len 200000 --max-num-seqs 2 --d 0,1 --pd-server

# å¯åŠ¨PDå®¢æˆ·ç«¯
# Rust
./run.sh --release --features cuda,nccl,flash-attn -- --d 2,3 --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --max-model-len 200000 --max-num-seqs 2 --server --port 8000 --pd-client

# Python
python3 -m vllm_rs.server --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --max-model-len 200000 --max-num-seqs 2 --d 2,3 --port 8000 --pd-client

# PD Serverä¸Clientå¯åŠ¨æ—¶çš„æ¨¡å‹åŠRankæ•°é‡ï¼ˆå¡æ•°ï¼‰éœ€è¦ä¸€è‡´ï¼Œå¯ä¸ºç›¸åŒæ¨¡å‹çš„ä¸åŒæ ¼å¼ï¼ˆä¾‹å¦‚æœåŠ¡å™¨æœªé‡åŒ–Safetensor, å®¢æˆ·ç«¯GGUFï¼‰
# å¦‚æœæŒ‡å®šäº† `--pd-url`ï¼ˆä¾‹å¦‚ serverç«¯: 0.0.0.0:8100, clientç«¯: server_ip:8100ï¼‰ï¼ŒPD æœåŠ¡å™¨/å®¢æˆ·ç«¯å°†å°è¯•ç»‘å®šæˆ–è¿æ¥åˆ°è¯¥åœ°å€ï¼Œ
# å®¢æˆ·ç«¯å°†å°è¯•ä½¿ç”¨æŒ‡å®šçš„ URL è¿æ¥åˆ°æœåŠ¡å™¨ï¼ˆMetalå¹³å°ä¸æ”¯æŒLocalIPC, å¿…é¡»æä¾›pd-urlï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒæœåŠ¡å™¨å’Œå®¢æˆ·ç«¯å¯ä»¥éƒ¨ç½²åœ¨ä¸åŒçš„æœºå™¨ä¸Šã€‚
```
---

## ğŸ“˜ ä½¿ç”¨æ–¹æ³•ï¼ˆPythonï¼‰
### ğŸ“¦ ä»pipå®‰è£…
   ğŸ’¡ 1. CUDA compute capability < 8.0 GPUè®¾å¤‡ï¼ˆä¾‹å¦‚V100ï¼Œä¸æ”¯æŒflash-attnç‰¹æ€§ï¼‰ä¸Šéœ€è¦æ‰‹åŠ¨ç¼–è¯‘å®‰è£…
   
   ğŸ’¡ 2. é¢„ç¼–è¯‘åŒ…`context cache` ç‰¹æ€§ä¸ä¾èµ–äºFlash attention, å¦‚éœ€å¯ç”¨`flash-context`ç‰¹æ€§éœ€æ‰‹åŠ¨ç¼–è¯‘å®‰è£…
```shell
python3 -m pip install vllm_rs fastapi uvicorn
```

### ğŸŒâœ¨ API Server
   ğŸ’¡ä½ å¯ä»¥ä½¿ç”¨**ä»»ä½•å…¼å®¹ OpenAI API çš„å®¢æˆ·ç«¯**è¿›è¡Œäº¤äº’ã€‚

   ğŸ¤– <a href="python/ReadMe.md">è¿™é‡ŒåŒ…å«å®¢æˆ·ç«¯ä½¿ç”¨Context-cacheçš„æ³¨æ„äº‹é¡¹</a>
```bash
# å¯åŠ¨ OpenAI å…¼å®¹çš„ API æœåŠ¡ï¼ˆç›‘å¬ http://0.0.0.0:8000ï¼‰
# openai.base_url = "http://localhost:8000/v1/"
# openai.api_key = "EMPTY"

# æœ¬åœ°GGUFæ¨¡å‹æ–‡ä»¶ (`--f`)ï¼Œæ¯ä¸ªè¯·æ±‚é»˜è®¤æœ€å¤§è¾“å‡ºtokensï¼ˆ`--max-tokens`)ï¼Œå¯ç”¨FP8 KV Cacheï¼ˆ`--fp8-kvcache`ï¼Œç²¾åº¦ç•¥æœ‰æŸå¤±)
python -m vllm_rs.server --f /path/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --host 0.0.0.0 --port 8000 --max-tokens 32768 --max-model-len 128000 --fp8-kvcache

# ä½¿ç”¨Model IDåŠ è½½ (`--m`: model_id, `--f`: GGUFæ–‡ä»¶å)
python -m vllm_rs.server --m unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF --f Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --host 0.0.0.0 --port 8000

# å¤šGPUæ¨ç† (`--d`)
python -m vllm_rs.server --f /path/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --d 0,1 --host 0.0.0.0 --port 8000 --max-model-len 64000

# Safetensorsæ¨¡å‹å¤šGPUæ¨ç†ï¼ˆåŒæ—¶å°†æƒé‡é‡åŒ–ä¸ºQ4Kæ ¼å¼ï¼Œå¯ç”¨æœ€é•¿ä¸Šä¸‹æ–‡ï¼‰ï¼š
python -m vllm_rs.server --w /path/Qwen3-30B-A3B-Instruct-2507 --d 0,1 --host 0.0.0.0 --port 8000 --isq q4k --max-model-len 262144 --max-num-seqs 1

# GGUFæ¨¡å‹å¤šGPUæ¨ç†+ä¸Šä¸‹æ–‡ç¼“å­˜ (ç¼“å­˜ä¸Šä¸‹æ–‡ï¼Œé€šè¿‡OpenAI APIå‘èµ·è¯·æ±‚æ—¶åœ¨`extra_body`å­—æ®µé‡Œä¼ å…¥`session_id`ï¼Œ`session_id`åœ¨å¯¹è¯è¿‡ç¨‹ä¸­ä¿æŒä¸å˜ï¼Œæ–°å¯¹è¯éœ€è¦å¯ç”¨æ–°çš„`session_id`ï¼Œæ— éœ€æ”¹å˜å…¶å®ƒè®¾ç½®)
python -m vllm_rs.server --m unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF --f Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --d 0,1 --host 0.0.0.0 --port 8000 --max-model-len 64000 --max-num-seqs 8 --context-cache
```


### ğŸ¤–âœ¨ äº¤äº’å¼èŠå¤©ä¸æ‰¹å¤„ç†

```bash
# äº¤äº’å¼èŠå¤©
# ä½¿ç”¨model idåŠ è½½
python -m vllm_rs.chat --m unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF --f Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --fp8-kvcache

# æœ¬åœ°GGUFæ–‡ä»¶åŠ è½½åˆ°è®¾å¤‡2 (è®¾å¤‡åºå·ä¸º1ï¼Œ`--d 1`)
python -m vllm_rs.chat --d 1 --f /path/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf

# å°†æœªé‡åŒ–æ¨¡å‹åŠ è½½ä¸ºGGUFé‡åŒ–æ¨¡å‹ (ä¾‹å¦‚q4kæ ¼å¼)ï¼Œå¹¶å¯ç”¨æœ€é•¿ä¸Šä¸‹æ–‡ï¼ˆ262144 tokensï¼‰ï¼Œé€‚ç”¨äºä»»æ„å·²æ”¯æŒçš„æ¨¡å‹æ¶æ„
python -m vllm_rs.chat --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --max-model-len 262144 --max-num-seqs 1 --max-tokens 16384

# å¯ç”¨ä¸Šä¸‹æ–‡ç¼“å­˜ï¼ˆå¿«é€Ÿå“åº”è¯·æ±‚ï¼‰
python -m vllm_rs.chat --d 0 --m unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF --f Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --max-model-len 262144 --max-num-seqs 1 --context-cache

# ISQ q4k (macOS/Metalæ¨èï¼Œå¯é€‰`--context-cache`)
python -m vllm_rs.chat --w /path/Qwen3-0.6B --isq q4k

# æ‰¹é‡åŒæ­¥ç¤ºä¾‹
python -m vllm_rs.completion --f /path/qwq-32b-q4_k_m.gguf --d 0,1 --prompts "How are you? | How to make money?"

# æ‰¹é‡åŒæ­¥ç¤ºä¾‹ (å¤šGPU)
python -m vllm_rs.completion --w /home/GLM-4-9B-0414 --d 0,1 --batch 8 --max-model-len 1024 --max-tokens 1024
```

### ğŸ Python API
```python
from vllm_rs import Engine, EngineConfig, SamplingParams, Message
cfg = EngineConfig(weight_path="/path/Qwen3-8B-Q2_K.gguf", max_model_len=4096)
engine = Engine(cfg, "bf16")
params = SamplingParams(temperature=0.6, max_tokens=256)
prompt = engine.apply_chat_template([Message("user", "How are you?")], True)

# åŒæ­¥æ‰¹é‡ç”Ÿæˆ
outputs = engine.generate_sync([params,params], [prompt, prompt])
print(outputs)

params.session_id = xxx #ä¼ å…¥session_idä»¥ä½¿ç”¨ä¸Šä¸‹æ–‡ç¼“å­˜åŠŸèƒ½

# å•è¯·æ±‚æµå¼ç”Ÿæˆ
(seq_id, prompt_length, stream) = engine.generate_stream(params, prompt)
for item in stream:
    # item.datatype == "TOKEN"
    print(item.data)
```

## ğŸ”¨ ä»æºä»£ç ç¼–è¯‘å®‰è£…ï¼ˆå¯é€‰ï¼‰

> âš ï¸ å¯ç”¨ Flash Attentionï¼ˆCUDAï¼‰æ—¶ï¼Œé¦–æ¬¡ç¼–è¯‘å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ã€‚

> âš ï¸ å¯ç”¨ ä¸Šä¸‹æ–‡ç¼“å­˜æˆ–å¤šGPUæ¨ç†æ—¶ï¼Œéœ€è¦åŒæ—¶ç¼–è¯‘`Runner`ï¼ˆä½¿ç”¨`build.sh` æˆ– `run.sh`ï¼‰

### ğŸ› ï¸ ç¯å¢ƒè¦æ±‚

* å®‰è£… [Rust å·¥å…·é“¾](https://www.rust-lang.org/tools/install)
* **macOS** å¹³å°éœ€å®‰è£… [Xcode å‘½ä»¤è¡Œå·¥å…·](https://mac.install.guide/commandlinetools/)
* æ„å»º Python æ¥å£éœ€å®‰è£… [Maturin](https://github.com/PyO3/maturin)

### ç¼–è¯‘æ­¥éª¤
1. **å®‰è£… Maturin**

```bash
sudo apt install libssl-dev pkg-config -y # ç¼–è¯‘ä¾èµ– (Linux)
pip install maturin
pip install maturin[patchelf]  # Linux/Windows å¹³å°
```

2. **æ„å»º Python åŒ…**

```bash
# Naive CUDA (åªèƒ½ç”¨äºå•å¡æ¨ç†) 
maturin build --release --features cuda,python

# Naive CUDA (+CUDA Graph, å®éªŒé˜¶æ®µ)
maturin build --release --features cuda,graph,python

# CUDA (æ”¯æŒContext-cacheä¸FP8 KV Cacheï¼Œä¸ä½¿ç”¨Flash attention) 
./build.sh --release --features cuda,nccl,python

# CUDA (+Flash attentionï¼Œä»…prefillæ—¶å¯ç”¨) 
./build.sh --release --features cuda,nccl,flash-attn,python

# CUDA (+Flash attentionï¼Œprefill/decodingå‡ä½¿ç”¨Flash attentionï¼Œç¼–è¯‘æ—¶é—´æœ€é•¿) 
./build.sh --release --features cuda,nccl,flash-context,python

# macOSï¼ˆMetal, æ”¯æŒContext-cacheä¸FP8 KV Cacheï¼Œä½†ä¸æ”¯æŒå¤šGPUæ¨ç†ï¼‰
maturin build --release --features metal,python

```

3. **å®‰è£…æ„å»ºå¥½çš„åŒ…ä¸ä¾èµ–**

```bash
pip install target/wheels/vllm_rs-*-cp38-abi3-*.whl --force-reinstall
pip install fastapi uvicorn
```


### âš™ï¸ å‘½ä»¤è¡Œå‚æ•°è¯´æ˜

| å‚æ•°          | æè¿°                                     |       |
| ----------- | -------------------------------------- | ----- |
| `--m`       | Hugginfaceæ¨¡å‹ID (ç”¨äºä¸‹è½½)               |    |
| `--w`       | Safetensoræ¨¡å‹è·¯å¾„           |       |
| `--f`       | å½“æŒ‡å®šModel IDæ—¶ä¸ºGGUFæ–‡ä»¶åï¼Œæˆ–æœªæŒ‡å®šæ—¶ä¸ºGGUFæœ¬åœ°æ–‡ä»¶è·¯å¾„                 |    |
| `--d`       | è®¾å¤‡ IDï¼Œä¾‹å¦‚ `--d 0`                       |       |
| `--max-num-seqs`   | åŒæ—¶å¤„ç†çš„æœ€å¤§è¯·æ±‚æ•°ï¼ˆé»˜è®¤ `32`, macOSå¹³å°ä¸º`8`ï¼‰   |       |
| `--max-tokens`     | å•æ¬¡æœ€å¤§è¾“å‡º token æ•°ï¼ˆé»˜è®¤ `4096`ï¼Œä¸Šé™ä¸ºæ¨¡å‹æ”¯æŒçš„æœ€å¤§é•¿åº¦ï¼‰ |       |
| `--batch`     | ä»…ç”¨äºæ€§èƒ½ (å¯ç”¨åä¼šå¿½ç•¥ `max-num-seqs` ä¸ `prompts`) |    |
| `--prompts` | è¾“å…¥çš„ promptï¼Œå¤šä¸ªä½¿ç”¨ \| åˆ†éš” |
| `--dtype`   | KV ç¼“å­˜æ•°æ®ç±»å‹ï¼š`bf16`ï¼ˆé»˜è®¤ï¼‰ã€`f16` æˆ– `f32`     |       |
| `--isq`   | å°†æœªé‡åŒ–æ¨¡å‹åŠ è½½ä¸ºGGUFé‡åŒ–æ¨¡å‹ï¼Œå¯é€‰`q2k`, `q4k`  ç­‰   |       |
| `--temperature`   | é‡‡æ ·æ¸©åº¦ (sampling temperature)ï¼Œæ§åˆ¶è¾“å‡ºâ€œéšæœºæ€§/åˆ›é€ æ€§â€çš„ä¸€ä¸ªè¶…å‚æ•°ï¼Œä»‹äº0-1ä¹‹é—´  |       |
| `--top-k`   | top-k æ§åˆ¶æ¨¡å‹åœ¨æ¯ä¸€æ­¥åªä»å‰ k ä¸ªæœ€é«˜æ¦‚ç‡çš„è¯é‡ŒæŒ‘é€‰ï¼Œk è¶Šå° â†’ è¶Šç¨³å®šï¼›k è¶Šå¤§ â†’ è¶Šéšæœº   |       |
| `--top-p`   | top-p é‡‡æ ·æ ¹æ®æ¦‚ç‡é˜ˆå€¼é€‰æ‹©åŠ¨æ€æ•°é‡çš„å€™é€‰ï¼ŒèŒƒå›´æ˜¯ [0,1]ï¼Œå¸¸ç”¨åœ¨ 0.8 ~ 0.95   |       |
| `--presence-penalty` | å‡ºç°æƒ©ç½šï¼Œæ§åˆ¶æ¨¡å‹æ˜¯å¦é¿å…å†æ¬¡æåŠ`å·²ç»å‡ºç°è¿‡çš„è¯`ã€‚<br> æ•°å€¼èŒƒå›´ [-2, 2]ï¼Œæ­£å€¼è¶Šå¤§ â†’ è¶Šå€¾å‘å¼•å…¥æ–°è¯æ±‡ï¼›è´Ÿå€¼ â†’ è¶Šå€¾å‘é‡å¤å·²å‡ºç°çš„è¯ | |
| `--frequency-penalty` | é¢‘ç‡æƒ©ç½šï¼Œæ§åˆ¶æ¨¡å‹æ˜¯å¦å‡å°‘`é«˜é¢‘é‡å¤è¯`çš„å‡ºç°ã€‚<br> æ•°å€¼èŒƒå›´ [-2, 2]ï¼Œæ­£å€¼è¶Šå¤§ â†’ é‡å¤æ¬¡æ•°è¶Šå¤šçš„è¯æƒ©ç½šè¶Šå¼ºï¼›è´Ÿå€¼ â†’ è¶Šé¼“åŠ±é‡å¤ä½¿ç”¨åŒä¸€è¯ | |
| `--server`       | æœåŠ¡æ¨¡å¼ï¼Œé€‚ç”¨äºRust CLIï¼ŒPythonä½¿ç”¨ `python -m vllm.server`        |       |
| `--fp8-kvcache`       | ä½¿ç”¨FP8 KV Cache (flash-contextæ²¡æœ‰å¯ç”¨æ—¶ç”Ÿæ•ˆ)                 |    |
| `--cpu-mem-fold`       | CPU KV Cacheå¤§å° (ä¸GPU KV Cacheçš„ç™¾åˆ†æ¯”ï¼Œé»˜è®¤ 1.0ï¼Œå–å€¼0.1 - 10.0)              |    |
| `--pd-server`       | ä½¿ç”¨PDåˆ†ç¦»æ¨¡å¼æ—¶ï¼ŒæŒ‡å®šå½“å‰å®ä¾‹ä¸ºPDæœåŠ¡å™¨ï¼ˆæ­¤æœåŠ¡å™¨ä»…ç”¨äºPrefillï¼‰            |    |
| `--pd-client`       | ä½¿ç”¨PDåˆ†ç¦»æ¨¡å¼æ—¶ï¼ŒæŒ‡å®šå½“å‰å®ä¾‹ä¸ºPDå®¢æˆ·ç«¯ï¼ˆæ­¤å®¢æˆ·ç«¯å°†é•¿çš„ä¸Šä¸‹æ–‡Prefillè¯·æ±‚å‘é€ç»™PDæœåŠ¡å™¨å¤„ç†ï¼‰|    |
| `--pd-url`       |  ä½¿ç”¨PDåˆ†ç¦»æ¨¡å¼æ—¶ï¼ŒPDæœåŠ¡å™¨å®ä¾‹å¦‚æŒ‡å®špd-urlï¼Œåˆ™é€šè¿‡TCP/IPé€šä¿¡ï¼ˆé€‚ç”¨äºPDæœåŠ¡å™¨ä¸å®¢æˆ·ç«¯åœ¨ä¸åŒæœåŠ¡å™¨ï¼‰ |    |

## ğŸ—œï¸ å®æ—¶é‡åŒ–ï¼ˆGGUF æ ¼å¼è½¬æ¢ï¼‰

   ğŸ’¡ å°†ä»»æ„éé‡åŒ–æ¨¡å‹å®æ—¶é‡åŒ–åŠ è½½ä¸ºGGUFæ ¼å¼ï¼ŒæŒ‡å®š`--isq`éq4kã€q8_0æ—¶å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼š

```bash
# macOS
cargo run --release --features metal -- --w /path/Qwen3-0.6B/ --isq q4k --prompts "How are you today?"

# CUDA
cargo run --release --features cuda,flash-attn -- --w /path/Qwen3-8B/ --isq q4k --prompts "How are you today?"
```


## ğŸ“Œ é¡¹ç›®çŠ¶æ€

> ğŸš§ **é¡¹ç›®ä»åœ¨ç§¯æå¼€å‘ä¸­ï¼Œæ¥å£ä¸åŠŸèƒ½å¯èƒ½å‘ç”Ÿå˜æ›´ã€‚**

## ğŸ› ï¸ å¼€å‘è®¡åˆ’ï¼ˆTODOï¼‰

* [x] Metal å¹³å°æ”¯æŒæ‰¹é‡æ¨ç†
* [x] æ”¯æŒ GGUF æ ¼å¼
* [x] CUDA å¹³å° Flash Attention æ”¯æŒ
* [x] CUDA Graph
* [x] OpenAI API å…¼å®¹æœåŠ¡å™¨ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
* [x] æŒç»­æ‰¹å¤„ç†
* [x] å¤šå¡å¹¶è¡Œæ¨ç†ï¼ˆSafetensorsæ¨¡å‹ã€GPTQ/AWQåŠGGUFé‡åŒ–æ¨¡å‹ï¼‰
* [x] Metal/macOSå¹³å°Promptå¤„ç†åŠ é€Ÿ
* [x] åˆ†å—é¢„å¡«å……ï¼ˆChunked Prefillï¼‰
* [x] ä¸Šä¸‹æ–‡ç¼“å­˜ (ä½¿ç”¨`context-cache`å‚æ•°)
* [x] ä»Hugginface Hubä¸‹è½½å¹¶åŠ è½½æ¨¡å‹
* [ ] ä»ModelScopeä¸‹è½½å¹¶åŠ è½½ (ä¸­å›½å¤§é™†åœ°åŒº)
* [x] Metal/macOSå¹³å°ä¸Šä¸‹æ–‡ç¼“å­˜
* [x] FP8 KV Cache (CUDA)
* [x] FP8 KV Cache (Metal)
* [ ] FP8 KV Cache (with Flash-Attn)
* [ ] æ”¯æŒæ›´å¤šæ¨¡å‹ç±»å‹ï¼ˆGLM 4.6, Kimi K2 Thinkingç­‰ï¼‰
* [x] CPU KV Cache å¸è½½
* [x] PDï¼ˆPrefill/Decodeï¼‰åˆ†ç¦»ï¼ˆCUDAï¼‰
* [x] PDï¼ˆPrefill/Decodeï¼‰åˆ†ç¦»ï¼ˆMetalï¼‰

## ğŸ“š å‚è€ƒé¡¹ç›®

å‚è€ƒï¼š

* [Candle-vLLM](https://github.com/EricLBuehler/candle-vllm)
* Python nano-vllm é¡¹ç›®

---

ğŸ’¡ **å–œæ¬¢è¿™ä¸ªé¡¹ç›®ï¼Ÿæ¬¢è¿ â­ æ”¶è—å’Œå‚ä¸è´¡çŒ®ï¼**
