# ğŸš€ **vLLM.rs** â€“ ç”¨ Rust å®ç°çš„æç®€ vLLM

ä¸€ä¸ªæé€Ÿ âš¡ã€è½»é‡çš„ ğŸ¦€**Rust å®ç°ç‰ˆ vLLM**ã€‚

---

<p align="center">
  <a href="./ReadMe.md">English</a> |
  <a href="./ReadMe-CN.md">ç®€ä½“ä¸­æ–‡</a>
</p>

## âœ¨ ä¸»è¦ç‰¹æ€§

* ğŸ”§ **çº¯ Rust åç«¯** â€“ å®Œå…¨**ä¸ä¾èµ– PyTorch**
* ğŸš€ **é«˜æ€§èƒ½** (æ”¯æŒ**ä¸Šä¸‹æ–‡ç¼“å­˜ã€PDåˆ†ç¦»**) â€“ æ€§èƒ½ä¼˜äºPythonåŒç±»æ¨ç†æ¡†æ¶
* ğŸ§  **æç®€æ ¸å¿ƒ** â€“ æ ¸å¿ƒé€»è¾‘ä»… **<3000 è¡Œ** Rust ä»£ç 
* ğŸ’» **è·¨å¹³å°æ”¯æŒ** â€“ æ”¯æŒ **CUDA**ï¼ˆLinux/Windowsï¼‰ä¸ **Metal**ï¼ˆmacOSï¼‰
* ğŸ¤– **å†…ç½®API æœåŠ¡ä¸ChatGPTé£æ ¼ç½‘é¡µ** â€“ Rust åŸç”Ÿå®ç°çš„èŠå¤©ä¸ API/Web æœåŠ¡
* ğŸ”Œ **MCPé›†æˆ** â€“ Model Context Protocol å·¥å…·è°ƒç”¨æ”¯æŒ
* ğŸ“Š **Embeddingä¸åˆ†è¯å™¨API** â€“ å®Œæ•´çš„æ–‡æœ¬å¤„ç†æ”¯æŒ
* ğŸ **è½»é‡ Python æ¥å£** â€“ ä½¿ç”¨ PyO3 æ„å»ºçš„ Python èŠå¤©æ¥å£
* ğŸ¤ **æ¬¢è¿è´¡çŒ®** â€“ æ¬¢è¿æäº¤ PRã€é—®é¢˜æˆ–ç»™é¡¹ç›®ç‚¹äº® â­ï¼

---

## ğŸ“ˆ æ€§èƒ½

### ğŸ’¬ å¯¹è¯æ€§èƒ½

> **A100** (å•å¡, 40G)

| æ¨¡å‹ | æ ¼å¼ | å¤§å° | è¾“å‡ºé€Ÿåº¦ |
|------------------|---------------|----------|------------------------|
| Llama-3.1-8B | ISQ (BF16->Q4K) | 8B | **90.19** tokens/s |
| DeepSeek-R1-Distill-Llama-8B | Q2_K | 8B | **94.47** tokens/s |
| DeepSeek-R1-0528-Qwen3-8B | Q4_K_M | 8B | **95** tokens/s |
| GLM-4-9B-0414 | Q4_K_M | 9B | **70.38** tokens/s |
| QwQ-32B | Q4_K_M | 32B | **35.69** tokens/s |
| **Qwen3-30B-A3B** | Q4_K_M | **30B (MoE)** | **75.91** tokens/s  |

> vLLM.rs åœ¨ **Metal (Apple Silicon, M4)** ä¸Šçš„æ€§èƒ½

   | æ¨¡å‹ | å¹¶å‘æ•° | è¾“å‡ºTokens | è€—æ—¶ (s) | ååé‡ (tokens/s) |
   |------------------|--------|--------|---------|-------------|
   | Qwen3-0.6B (BF16) |  128  | 63488       | 83.13s    | 763.73     |
   | Qwen3-0.6B (BF16) |  32      | 15872       | 23.53s    | 674.43    |
   | Qwen3-0.6B (BF16) | 1       | 456       | 9.23s    | 49.42       |
   | Qwen3-4B (Q4_K_M)  | 1       | 1683       | 52.62s    | 31.98     |
   | Qwen3-8B (Q2_K)  | 1       | 1300       | 80.88s    | 16.07     |

æŸ¥çœ‹ [**å®Œæ•´æ€§èƒ½æµ‹è¯• â†’**](docs/performance.md)

## ğŸ§  æ”¯æŒçš„æ¨¡å‹æ¶æ„

* âœ… LLaMa ç³»åˆ—ï¼ˆLLaMa2ã€LLaMa3ï¼‰
* âœ… Qwen ç³»åˆ—ï¼ˆQwen2ã€Qwen3ï¼‰
* âœ… Qwen2 Moe ç³»åˆ—ï¼ˆä½¿ç”¨Qwen3 MoEæµç¨‹+å…±äº«ä¸“å®¶å±‚ï¼‰
* âœ… Qwen3 MoE ç³»åˆ—
* âœ… Mistral v1, v2
* âœ… Mistral-3 VL Reasoning (3B, 8B, 14B, å¤šæ¨¡æ€)
* âœ… GLM4 (0414ç‰ˆæœ¬, **éChatGLM**)
* âœ… Gemma3 (å¤šæ¨¡æ€ï¼Œä¸æ”¯æŒFlash Attention)
* âœ… Qwen3-VL (Dense, å¤šæ¨¡æ€)

æ”¯æŒ **Safetensor** (åŒ…å«GPTQ, AWQé‡åŒ–æ ¼å¼) å’Œ **GGUF** æ ¼å¼ã€‚

---
## ğŸ“š æ–‡æ¡£
- [å¿«é€Ÿå¼€å§‹](docs/get_started.md)
- [MCPé›†æˆä¸å·¥å…·è°ƒç”¨](docs/mcp_tool_calling.md)
- [Embedding](docs/embeddings.md)
- [å¤šæ¨¡æ€ (Qwen3-VL, Gemma3, Mistral3-VL)](docs/multimodal.md)
- [ä¸Šä¸‹æ–‡ç¼“å­˜](docs/context-cache.md)
- [Ruståº“](docs/rust_crate.md)
- [Tokenize/Detokenize](docs/tokenize.md)
- [æ€§èƒ½æµ‹è¯•](docs/performance.md)


## ğŸ“˜ ä½¿ç”¨æ–¹æ³•ï¼ˆPythonï¼‰
### ğŸ“¦ ä»pipå®‰è£…
   ğŸ’¡ 1. CUDA compute capability < 8.0 GPUè®¾å¤‡ï¼ˆä¾‹å¦‚V100ï¼Œä¸æ”¯æŒflash-attnç‰¹æ€§ï¼‰ä¸Šéœ€è¦æ‰‹åŠ¨ç¼–è¯‘å®‰è£…ï¼ˆæˆ–ç›´æ¥ä½¿ç”¨Rustæ–¹å¼ï¼‰
   
   ğŸ’¡ 2. é¢„ç¼–è¯‘åŒ…`context cache` ä¾èµ–äºFlash attention, å¦‚éœ€FP8 KvCacheï¼Œè¯·é‡æ–°ç¼–è¯‘å¹¶å»é™¤`flash-context`ç‰¹æ€§
   
   â¬‡ï¸ ä»…é€‚ç”¨äº `å• GPUï¼ˆCUDAï¼‰`çš„å…¶ä»–é¢„ç¼–è¯‘ Python åŒ…ï¼ˆå‹ç¼©åŒ…ï¼‰ï¼Œä¸‹è½½é“¾æ¥ï¼š
   1. [ä¸å« `nccl` åŠŸèƒ½çš„åŒ…](https://github.com/guoqingbao/vllm.rs/releases/download/v0.5.4/vllm_rs-0.5.8-cp38-abi3-no-NCCL.tar.gz)

   2. [ä¸å« `nccl` å’Œ `flash-attn` åŠŸèƒ½çš„åŒ…](https://github.com/guoqingbao/vllm.rs/releases/download/v0.5.4/vllm_rs-0.5.8-cp38-abi3-no-NCCL-and-flash-attn.tar.gz)
```shell
# CUDAå¹³å°éœ€å®‰è£…NCCLåº“ï¼ˆå•å¡ä½¿ç”¨Rustæ¨¡å¼å¯ä¸å¿…å®‰è£…NCCLï¼‰
python3 -m pip install vllm_rs
```

### ğŸŒâœ¨ API Server
   ğŸ’¡ä½ å¯ä»¥ä½¿ç”¨**ä»»ä½•å…¼å®¹ OpenAI API çš„å®¢æˆ·ç«¯**è¿›è¡Œäº¤äº’
   
   ğŸ’¡ä½¿ç”¨`--ui-server`ä¼šåŒæ—¶å¯åŠ¨ChatGPTé£æ ¼ç½‘é¡µ, æ­¤æ—¶æ— éœ€å…¶å®ƒå®¢æˆ·ç«¯ã€‚

   ğŸ’¡å¦‚é•¿æ–‡æœ¬è¯·æ±‚å¯¼è‡´å½“å‰ç”Ÿæˆè¿‡ç¨‹å¡é¡¿ï¼Œè¯·ä½¿ç”¨ **Rust PD Server**æ–¹æ¡ˆ ï¼ˆè§**PDåˆ†ç¦»**ï¼‰

  <details open>
    <summary>å•å¡ + GGUFæ¨¡å‹</summary>

  ```bash
  # ä»¥ä¸‹å‘½ä»¤å°†åŒæ—¶å¯åŠ¨ API Server å’Œ Web Serverï¼ˆChatGPT ç±»ç½‘é¡µç•Œé¢ï¼‰
  # è¯·é€šè¿‡æ–¹å‘é”®é€‰æ‹©è®¿é—®æ¨¡å¼ï¼ˆæœ¬åœ°è®¿é—® Local Access / è¿œç¨‹è®¿é—® Remote Accessï¼‰ï¼›
  # è‹¥ Server ä¸è¿è¡Œç½‘é¡µçš„å®¢æˆ·ç«¯ä¸åœ¨åŒä¸€ä¸»æœºä¸Šï¼Œå»ºè®®é€‰æ‹©è¿œç¨‹è®¿é—® Remote Accessã€‚
  # API Server åœ°å€ç¤ºä¾‹: http://<IP>:8000/v1/ï¼ˆAPI Key: æ— ï¼‰
  # Web Serverï¼ˆç‚¹å‡»æ‰“å¼€ ChatGPT ç±»ç½‘é¡µï¼‰: http://<IP>:8001
  ```

  ```bash
  # CUDA
  python3 -m vllm_rs.server --m unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF --f Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --kv-fraction 0.7 --ui-server --context-cache
  # Metal/MacOS
  python3 -m vllm_rs.server --m unsloth/Qwen3-4B-GGUF --f Qwen3-4B-Q4_K_M.gguf --ui-server --max-model-len 32768 --context-cache
   ```
  </details>

   <details open>
    <summary>å¤šGPU + æœ¬åœ°GGUFæ¨¡å‹</summary>

   ```bash
   python3 -m vllm_rs.server --f /path/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --d 0,1 --ui-server --context-cache
   ```
  </details>

<details open>
    <summary>å¤šæ¨¡æ€æ¨¡å‹ (Qwen3 VL, +å›¾ç‰‡)</summary>

```bash
# ä½¿ç”¨å†…ç½®ChatUIä¸Šä¼ æˆ–æåŠå›¾ç‰‡url (æ ¼å¼ '.bmp', '.gif', '.jpeg', '.png', '.tiff', or '.webp')
python3 -m vllm_rs.server --m Qwen/Qwen3-VL-8B-Instruct --ui-server --context-cache
```

  </details>

   <details open>
    <summary>å°†æœªé‡åŒ–æ¨¡å‹åŠ è½½ä¸ºGGUFæ¨¡å‹</summary>

   ```bash
   # åŒæ—¶å°†æƒé‡é‡åŒ–ä¸ºQ4Kæ ¼å¼ï¼Œå¯ç”¨æœ€é•¿ä¸Šä¸‹æ–‡ï¼š
   python3 -m vllm_rs.server --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --d 0,1 --port 8000 --max-model-len 262144 --max-num-seqs 1 --ui-server --context-cache
   ```
  </details>

  <details>
    <summary>è¿è¡ŒGPTQ/AWQ Marlinå…¼å®¹æ¨¡å‹</summary>

```bash
python3 -m vllm_rs.server --w /home/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4-Marlin
```
  </details>

### ğŸ¤–âœ¨ äº¤äº’å¼èŠå¤©ä¸æ‰¹å¤„ç†

  <details open>
    <summary>ä½¿ç”¨Huggingface æ¨¡å‹idåŠ è½½</summary>

   ```bash
   # é»˜è®¤ä½¿ç”¨Context-cache
   python3 -m vllm_rs.chat --m unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF --f Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf
   ```
  </details>

  <details open>
    <summary>å°†æœªé‡åŒ–æ¨¡å‹åŠ è½½ä¸ºGGUFé‡åŒ–æ¨¡å‹</summary>

   ```bash
   # å¹¶å¯ç”¨æœ€é•¿ä¸Šä¸‹æ–‡ï¼ˆ262144 tokensï¼‰
   python3 -m vllm_rs.chat --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --max-model-len 262144
   ```
  </details>

  <details>
    <summary>æ‰¹é‡åŒæ­¥ç¤ºä¾‹</summary>

   ```bash
   python3 -m vllm_rs.completion --f /path/qwq-32b-q4_k_m.gguf --d 0,1 --prompts "How are you? | How to make money?"
   ```

   ```bash
   python3 -m vllm_rs.completion --w /home/GLM-4-9B-0414 --d 0,1 --batch 8 --max-model-len 1024 --max-tokens 1024
   ```
  </details>

   ğŸ¤– <a href="python/ReadMe.md">è¿™é‡ŒåŒ…å«å®¢æˆ·ç«¯ä½¿ç”¨Context-cacheçš„æ³¨æ„äº‹é¡¹</a>


## ğŸ“˜ ä½¿ç”¨æ–¹æ³•ï¼ˆRustï¼‰

ä½¿ç”¨ `--i` å¯ç”¨äº¤äº’æ¨¡å¼ ğŸ¤–ï¼Œ`--server` å¯ç”¨æœåŠ¡æ¨¡å¼ ğŸŒï¼Œ`--m`æŒ‡å®šHuggingfaceæ¨¡å‹ï¼Œæˆ–`--w` æŒ‡å®šæœ¬åœ°Safetensorsæ¨¡å‹è·¯å¾„ æˆ–`--f` æŒ‡å®šGGUFæ¨¡å‹æ–‡ä»¶ï¼š

> Chatæ¨¡å¼
  <details open>
    <summary>å•å¡æ¨ç† + å†…ç½®Context Cache</summary>

   ```bash
   # CUDA
   cargo run --release --features cuda -- --i --m unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF --f Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --kv-fraction 0.8
   # Metal/MacOS (å½“MacOS GPU å†…å­˜ä½¿ç”¨è¶…è¿‡95%æ—¶å“åº”ä¼šéå¸¸æ…¢ï¼Œä½¿ç”¨æ›´å°çš„`--max-model-len` æˆ– `--kv-fraction`å‡å°‘æ˜¾å­˜å ç”¨)
   cargo run --release --features metal -- --i --m Qwen/Qwen3-4B-GGUF --f Qwen3-4B-Q4_K_M.gguf
   ```
  </details>

  <details open>
    <summary>å¤šæ¨¡æ€ (Mistral-3 VL)</summary>

```bash
# ä½¿ç”¨å†…ç½®çš„ChatUIä¸Šä¼ å›¾ç‰‡æˆ–å¯¹è¯ä¸­æåˆ°æŒ‡å®šå›¾ç‰‡URLåœ°å€ (URLç»“å°¾ä¸º '.bmp', '.gif', '.jpeg', '.png', '.tiff', or '.webp')
python3 -m vllm_rs.server --m mistralai/Ministral-3-3B-Reasoning-2512 --ui-server
```

  </details>

  <details open>
    <summary>å¤šå¡æ¨ç† + CUDA Graph + Flash attention + FP8 kvcache</summary>

   ```bash
   # éœ€ä½¿ç”¨run.shç”Ÿæˆç‹¬ç«‹runner
  ./run.sh --release --features cuda,nccl,graph,flash-attn --i --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507 --port 8000 --fp8-kvcache
   ```
  </details>

---

> å¤šå¡æ¨ç† API Server + **ChatGPTé£æ ¼ç½‘é¡µ**

  <details open>
    <summary>è¿è¡Œæœªé‡åŒ–Qwen3-30B-A3Bæ¨¡å‹</summary>

   ```bash
   # å»é™¤ `flash-context`å³å¯åœ¨V100ä¸Šä½¿ç”¨ï¼Œè¿›ä¸€æ­¥å»é™¤`graph`ç‰¹æ€§å³å¯åœ¨Metal/MacOSä¸Šä½¿ç”¨
   ./run.sh --release --features cuda,nccl,graph,flash-context --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507 --max-num-seqs 2 --ui-server --port 8000
   ```
  </details>

   <details open>
    <summary>å¤šå¡è¿è¡ŒQwen3-30B-A3Bé‡åŒ–æ¨¡å‹</summary>

   ```bash
   ./run.sh --release --features cuda,nccl,graph,flash-attn --ui-server --d 0,1 --f /path/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --context-cache
   ```
  </details>

   <details>
    <summary>å°†æœªé‡åŒ–Qwen3-30B-A3Bæ¨¡å‹è¿è¡Œä¸ºQ4Ké‡åŒ–æ¨¡å‹ï¼ŒåŒæ—¶ä½¿ç”¨FP8 KVCache</summary>

   ```bash
   # å»é™¤`flash-context`ä»¥ä½¿ç”¨fp8 kvcache
   ./run.sh --release --features cuda,nccl,flash-attn --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --server --port 8000 --fp8-kvcache
   ```
  </details>

   <details>
    <summary>é«˜æ€§èƒ½Prefillæ–¹æ¡ˆ</summary>

   ä½¿ç”¨Flash Attentionåšcontext-cacheåŠdecodingï¼ˆéœ€è¦Ampere+ç¡¬ä»¶ï¼Œç¼–è¯‘è€—æ—¶æ—¶é•¿ï¼Œé•¿æ–‡æœ¬Prefillæ€§èƒ½æœ€é«˜ï¼‰
   ```bash
   ./run.sh --release --features cuda,nccl,flash-attn,flash-context --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507 --ui-server --port 8000 --context-cache
   ```
  </details>

---

> MacOS/Metalå¹³å°

  <details open>
    <summary>è¿è¡ŒQ2Ké‡åŒ–æ¨¡å‹</summary>

   ```bash
   # ä½¿ç”¨ `--fp8-kvcache`å‚æ•°å¯ç”¨fp8 kvcache (ç²¾åº¦ä¸é€Ÿåº¦ç•¥æœ‰ä¸‹é™)
   cargo run --release --features metal -- --ui-server --m Qwen/Qwen3-8B-GGUF --f Qwen3-8B-Q4_K_M.gguf --context-cache --fp8-kvcache
   ```
  </details>

  <details>
    <summary>å°†æœªé‡åŒ–æ¨¡å‹è¿è¡Œä¸ºQ6Ké‡åŒ–æ¨¡å‹ï¼ŒåŒæ—¶ä½¿ç”¨Context-cache</summary>

   ```bash
   cargo run --release --features metal -- --ui-server --w /path/Qwen3-0.6B --isq q6k
   ```
  </details>

---

## ğŸ”Œ MCPé›†æˆ (å·¥å…·è°ƒç”¨)

é€šè¿‡Model Context Protocolè®©LLMè°ƒç”¨å¤–éƒ¨å·¥å…·ã€‚æŸ¥çœ‹ [**MCPæ–‡æ¡£ â†’**](docs/mcp_tool_calling.md)

```bash
# å¯åŠ¨æ—¶é…ç½®MCPæ–‡ä»¶ç³»ç»ŸæœåŠ¡å™¨
cargo run --release --features metal -- --m Qwen/Qwen3-8B-GGUF --f Qwen3-8B-Q4_K_M.gguf --ui-server --context-cache \
  --mcp-command npx \
  --mcp-args=-y,@modelcontextprotocol/server-filesystem,~/

# æˆ–ä½¿ç”¨é…ç½®æ–‡ä»¶é…ç½®å¤šä¸ªMCPæœåŠ¡å™¨
./run.sh --release --features cuda --m unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF --f Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --ui-server --context-cache \
  --mcp-config ./mcp.json
```

---

## ğŸ”€ Prefill-decode åˆ†ç¦»ï¼ˆPDåˆ†ç¦»ï¼‰

  <details>
    <summary>å¯åŠ¨PDæœåŠ¡å™¨</summary>
   Metal/MacOSå¹³å°æˆ–PDæœåŠ¡å™¨ä¸PDå®¢æˆ·ç«¯ä¸åœ¨åŒä¸€OSï¼ŒæœåŠ¡å™¨ä¸å®¢æˆ·ç«¯éœ€è¦åŒæ—¶æŒ‡å®š`--pd-url`ï¼ˆä¾‹å¦‚0.0.0.0:8100ï¼‰

   æ— éœ€æŒ‡å®š`port`ï¼Œå› ä¸ºæ­¤æœåŠ¡å™¨ä¸ç›´æ¥æ¥æ”¶ç”¨æˆ·è¯·æ±‚ï¼ŒKvCacheå¤§å°ç”±`--max-model-len`å’Œ`--max-num-seqs`æ§åˆ¶ã€‚
   ```bash
   # PDæœåŠ¡å™¨ä½¿ç”¨`flash-context`åŠ å¿«å¤„ç†é•¿æ–‡æœ¬prefillï¼ˆPDæœåŠ¡å™¨å¯åŠ¨éé‡åŒ–æ¨¡å‹å¯è·å¾—æœ€ä½³ååç‡ï¼‰
   ./run.sh --release --features cuda,nccl,flash-context --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507 --pd-server
   ```

   PDæœåŠ¡å™¨è¿˜å¯ä½¿ç”¨é¢„ç¼–è¯‘PythonåŒ…å¯åŠ¨ (ä¾èµ–ï¼špip install vllm_rs)
   ```bash
   python3 -m vllm_rs.server --w /path/Qwen3-30B-A3B-Instruct-2507 --d 0,1 --pd-server
   ```
  </details>

  <details>
    <summary>å¯åŠ¨PDå®¢æˆ·ç«¯</summary>

   ```bash
   ./run.sh --release --features cuda,nccl,flash-context --d 2,3 --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --ui-server --port 8000 --pd-client
   ```

  PDå®¢æˆ·ç«¯è¿˜å¯ä½¿ç”¨é¢„ç¼–è¯‘PythonåŒ…å¯åŠ¨ (ä¾èµ–ï¼špip install vllm_rs)
  ```bash
   python3 -m vllm_rs.server --d 2,3 --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --ui-server --port 8000 --pd-client
   ```
  </details>

  <details>
    <summary>å•æœºå¤šä¸ªDockers/å¤šæœºé…ç½®</summary>

   PD Serverä¸Clientå¯åŠ¨æ—¶çš„æ¨¡å‹åŠRankæ•°é‡ï¼ˆå¡æ•°ï¼‰éœ€è¦ä¸€è‡´ï¼Œå¯ä¸ºç›¸åŒæ¨¡å‹çš„ä¸åŒæ ¼å¼ï¼ˆä¾‹å¦‚æœåŠ¡å™¨æœªé‡åŒ–Safetensor, å®¢æˆ·ç«¯GGUFï¼‰
   å¦‚æœæŒ‡å®šäº† `--pd-url`ï¼ˆä¾‹å¦‚ serverç«¯: 0.0.0.0:8100, clientç«¯: server_ip:8100ï¼‰ï¼ŒPD æœåŠ¡å™¨/å®¢æˆ·ç«¯å°†å°è¯•ç»‘å®šæˆ–è¿æ¥åˆ°è¯¥åœ°å€ï¼Œ
   å®¢æˆ·ç«¯å°†å°è¯•ä½¿ç”¨æŒ‡å®šçš„ URL è¿æ¥åˆ°æœåŠ¡å™¨ï¼ˆMetalå¹³å°ä¸æ”¯æŒLocalIPC, å¿…é¡»æä¾›pd-urlï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒæœåŠ¡å™¨å’Œå®¢æˆ·ç«¯å¯ä»¥éƒ¨ç½²åœ¨ä¸åŒçš„æœºå™¨ä¸Šã€‚
   å•æœºå¤šå¡ï¼ŒPDæœåŠ¡å™¨ä¸å®¢æˆ·ç«¯è¿è¡Œäºä¸åŒDockerï¼Œéœ€è¦é…ç½®Dockerå¯åŠ¨å‚æ•° `--ipc=host`
  </details>

---

## ğŸ“½ï¸ æ¼”ç¤ºè§†é¢‘

ğŸ‰ è§‚çœ‹é¡¹ç›®è¿è¡Œæ¼”ç¤ºï¼š
<video src="https://github.com/user-attachments/assets/7fc6aa0b-78ac-4323-923f-d761dd12857f" width="1000px"></video>


## ğŸ”¨ ä»æºä»£ç ç¼–è¯‘å®‰è£…ï¼ˆå¯é€‰ï¼‰

> âš ï¸ å¯ç”¨ Flash Attentionï¼ˆCUDAï¼‰æ—¶ï¼Œé¦–æ¬¡ç¼–è¯‘å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ã€‚

> âš ï¸ å¯ç”¨ ä¸Šä¸‹æ–‡ç¼“å­˜æˆ–å¤šGPUæ¨ç†æ—¶ï¼Œéœ€è¦åŒæ—¶ç¼–è¯‘`Runner`ï¼ˆä½¿ç”¨`build.sh`ç¼–è¯‘ æˆ– `run.sh`è¿è¡Œï¼‰

### ğŸ› ï¸ ç¯å¢ƒè¦æ±‚

* å®‰è£… [Rust å·¥å…·é“¾](https://www.rust-lang.org/tools/install)
* **macOS** å¹³å°éœ€å®‰è£… [Xcode å‘½ä»¤è¡Œå·¥å…·](https://mac.install.guide/commandlinetools/)
* æ„å»º Python æ¥å£éœ€å®‰è£… [Maturin](https://github.com/PyO3/maturin)

### ç¼–è¯‘æ­¥éª¤
1. **å®‰è£… Maturin**

```bash
sudo apt install libssl-dev pkg-config -y # ç¼–è¯‘ä¾èµ– (Linux)
pip install maturin
pip install maturin[patchelf]  # Linux/Windows å¹³å°
```

2. **æ„å»º Python åŒ…**

```bash
# Naive CUDA (åªèƒ½ç”¨äºå•å¡æ¨ç†) 
maturin build --release --features cuda,python

# Naive CUDA (+CUDA Graph, å®éªŒé˜¶æ®µ)
maturin build --release --features cuda,graph,python

# CUDA (æ”¯æŒContext-cacheä¸FP8 KV Cacheï¼Œä¸ä½¿ç”¨Flash attention) 
./build.sh --release --features cuda,nccl,python

# CUDA (+Flash attentionï¼Œä»…prefillæ—¶å¯ç”¨) 
./build.sh --release --features cuda,nccl,flash-attn,python

# CUDA (+Flash attentionï¼Œprefill/decodingå‡ä½¿ç”¨Flash attentionï¼Œç¼–è¯‘æ—¶é—´æœ€é•¿) 
./build.sh --release --features cuda,nccl,flash-context,python

# macOSï¼ˆMetal, æ”¯æŒContext-cacheä¸FP8 KV Cacheï¼Œä½†ä¸æ”¯æŒå¤šGPUæ¨ç†ï¼‰
maturin build --release --features metal,python

```

3. **å®‰è£…æ„å»ºå¥½çš„åŒ…ä¸ä¾èµ–**

```bash
pip install target/wheels/vllm_rs-*-cp38-abi3-*.whl --force-reinstall
```


### âš™ï¸ å‘½ä»¤è¡Œå‚æ•°è¯´æ˜

| å‚æ•°          | æè¿°                                     |
| ----------- | -------------------------------------- |
| `--m`       | Hugginfaceæ¨¡å‹ID (ç”¨äºä¸‹è½½)               |
| `--w`       | Safetensoræ¨¡å‹è·¯å¾„           |
| `--f`       | å½“æŒ‡å®šModel IDæ—¶ä¸ºGGUFæ–‡ä»¶åï¼Œæˆ–æœªæŒ‡å®šæ—¶ä¸ºGGUFæœ¬åœ°æ–‡ä»¶è·¯å¾„                 |
| `--d`       | è®¾å¤‡ IDï¼Œä¾‹å¦‚ `--d 0`                       |
| `--max-num-seqs`   | åŒæ—¶å¤„ç†çš„æœ€å¤§è¯·æ±‚æ•°ï¼ˆé»˜è®¤ `32`, macOSå¹³å°ä¸º`8`ï¼‰   |
| `--max-tokens`     | å•æ¬¡æœ€å¤§è¾“å‡º token æ•°ï¼ˆé»˜è®¤ `4096`ï¼Œä¸Šé™ä¸ºæ¨¡å‹æ”¯æŒçš„æœ€å¤§é•¿åº¦ï¼‰ |
| `--batch`     | ä»…ç”¨äºæ€§èƒ½ (å¯ç”¨åä¼šå¿½ç•¥ `max-num-seqs` ä¸ `prompts`) |
| `--prompts` | è¾“å…¥çš„ promptï¼Œå¤šä¸ªä½¿ç”¨ \| åˆ†éš” |
| `--dtype`   | KV ç¼“å­˜æ•°æ®ç±»å‹ï¼š`bf16`ï¼ˆé»˜è®¤ï¼‰ã€`f16` æˆ– `f32`     |
| `--isq`   | å°†æœªé‡åŒ–æ¨¡å‹åŠ è½½ä¸ºGGUFé‡åŒ–æ¨¡å‹ï¼Œå¯é€‰`q2k`, `q4k`  ç­‰   |
| `--temperature`   | é‡‡æ ·æ¸©åº¦ (sampling temperature)ï¼Œæ§åˆ¶è¾“å‡º"éšæœºæ€§/åˆ›é€ æ€§"çš„ä¸€ä¸ªè¶…å‚æ•°ï¼Œä»‹äº0-1ä¹‹é—´  |
| `--top-k`   | top-k æ§åˆ¶æ¨¡å‹åœ¨æ¯ä¸€æ­¥åªä»å‰ k ä¸ªæœ€é«˜æ¦‚ç‡çš„è¯é‡ŒæŒ‘é€‰ï¼Œk è¶Šå° â†’ è¶Šç¨³å®šï¼›k è¶Šå¤§ â†’ è¶Šéšæœº   |
| `--top-p`   | top-p é‡‡æ ·æ ¹æ®æ¦‚ç‡é˜ˆå€¼é€‰æ‹©åŠ¨æ€æ•°é‡çš„å€™é€‰ï¼ŒèŒƒå›´æ˜¯ [0,1]ï¼Œå¸¸ç”¨åœ¨ 0.8 ~ 0.95   |
| `--presence-penalty` | å‡ºç°æƒ©ç½šï¼Œæ§åˆ¶æ¨¡å‹æ˜¯å¦é¿å…å†æ¬¡æåŠ`å·²ç»å‡ºç°è¿‡çš„è¯`ã€‚<br> æ•°å€¼èŒƒå›´ [-2, 2]ï¼Œæ­£å€¼è¶Šå¤§ â†’ è¶Šå€¾å‘å¼•å…¥æ–°è¯æ±‡ï¼›è´Ÿå€¼ â†’ è¶Šå€¾å‘é‡å¤å·²å‡ºç°çš„è¯ |
| `--frequency-penalty` | é¢‘ç‡æƒ©ç½šï¼Œæ§åˆ¶æ¨¡å‹æ˜¯å¦å‡å°‘`é«˜é¢‘é‡å¤è¯`çš„å‡ºç°ã€‚<br> æ•°å€¼èŒƒå›´ [-2, 2]ï¼Œæ­£å€¼è¶Šå¤§ â†’ é‡å¤æ¬¡æ•°è¶Šå¤šçš„è¯æƒ©ç½šè¶Šå¼ºï¼›è´Ÿå€¼ â†’ è¶Šé¼“åŠ±é‡å¤ä½¿ç”¨åŒä¸€è¯ |
| `--server`       | æœåŠ¡æ¨¡å¼ï¼Œé€‚ç”¨äºRust CLIï¼ŒPythonä½¿ç”¨ `python -m vllm.server`        |
| `--fp8-kvcache`       | ä½¿ç”¨FP8 KV Cache (flash-contextæ²¡æœ‰å¯ç”¨æ—¶ç”Ÿæ•ˆ)                 |
| `--cpu-mem-fold`       | CPU KV Cacheå¤§å° (ä¸GPU KV Cacheçš„ç™¾åˆ†æ¯”ï¼Œé»˜è®¤ 0.5ï¼Œå–å€¼0.1 - 10.0)              |
| `--pd-server`       | ä½¿ç”¨PDåˆ†ç¦»æ¨¡å¼æ—¶ï¼ŒæŒ‡å®šå½“å‰å®ä¾‹ä¸ºPDæœåŠ¡å™¨ï¼ˆæ­¤æœåŠ¡å™¨ä»…ç”¨äºPrefillï¼‰            |
| `--pd-client`       | ä½¿ç”¨PDåˆ†ç¦»æ¨¡å¼æ—¶ï¼ŒæŒ‡å®šå½“å‰å®ä¾‹ä¸ºPDå®¢æˆ·ç«¯ï¼ˆæ­¤å®¢æˆ·ç«¯å°†é•¿çš„ä¸Šä¸‹æ–‡Prefillè¯·æ±‚å‘é€ç»™PDæœåŠ¡å™¨å¤„ç†ï¼‰|
| `--pd-url`       |  ä½¿ç”¨PDåˆ†ç¦»æ¨¡å¼æ—¶ï¼ŒPDæœåŠ¡å™¨å®ä¾‹å¦‚æŒ‡å®špd-urlï¼Œåˆ™é€šè¿‡TCP/IPé€šä¿¡ï¼ˆé€‚ç”¨äºPDæœåŠ¡å™¨ä¸å®¢æˆ·ç«¯åœ¨ä¸åŒæœåŠ¡å™¨ï¼‰ |
| `--ui-server`       |  æœåŠ¡æ¨¡å¼: å¯åŠ¨APIæœåŠ¡ï¼ŒåŒæ—¶å¯åŠ¨ChatGPTé£æ ¼çš„å†…ç½®å¯¹è¯ç½‘é¡µæœåŠ¡ |
| `--kv-fraction`       |  ç”¨äºæ§åˆ¶KVCacheä½¿ç”¨é‡ (æ¨¡å‹åŠ è½½åå‰©ä½™å¯ç”¨GPUæ˜¾å­˜çš„ç™¾åˆ†æ¯”) |
| `--context-cache`   | å¯ç”¨ä¸Šä¸‹æ–‡ç¼“å­˜ï¼Œç”¨äºå¤šè½®å¯¹è¯ |

### MCPé…ç½®å‚æ•°

| å‚æ•° | æè¿° |
|------|------|
| `--mcp-command` | å•ä¸ªMCPæœåŠ¡å™¨å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„ |
| `--mcp-args` | MCPæœåŠ¡å™¨å‚æ•°ï¼ˆé€—å·åˆ†éš”ï¼‰ |
| `--mcp-config` | å¤šä¸ªMCPæœåŠ¡å™¨çš„JSONé…ç½®æ–‡ä»¶è·¯å¾„ |

## ğŸ“Œ é¡¹ç›®çŠ¶æ€

> ğŸš§ **é¡¹ç›®ä»åœ¨ç§¯æå¼€å‘ä¸­ï¼Œæ¥å£ä¸åŠŸèƒ½å¯èƒ½å‘ç”Ÿå˜æ›´ã€‚**

## ğŸ› ï¸ å¼€å‘è®¡åˆ’ï¼ˆTODOï¼‰

* [x] Metal å¹³å°æ”¯æŒæ‰¹é‡æ¨ç†
* [x] æ”¯æŒ GGUF æ ¼å¼
* [x] CUDA å¹³å° Flash Attention æ”¯æŒ
* [x] CUDA Graph
* [x] OpenAI API å…¼å®¹æœåŠ¡å™¨ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
* [x] æŒç»­æ‰¹å¤„ç†
* [x] å¤šå¡å¹¶è¡Œæ¨ç†ï¼ˆSafetensorsæ¨¡å‹ã€GPTQ/AWQåŠGGUFé‡åŒ–æ¨¡å‹ï¼‰
* [x] Metal/macOSå¹³å°Promptå¤„ç†åŠ é€Ÿ
* [x] åˆ†å—é¢„å¡«å……ï¼ˆChunked Prefillï¼‰
* [x] ä¸Šä¸‹æ–‡ç¼“å­˜ (ä½¿ç”¨`context-cache`å‚æ•°)
* [x] ä»Hugginface Hubä¸‹è½½å¹¶åŠ è½½æ¨¡å‹
* [ ] ä»ModelScopeä¸‹è½½å¹¶åŠ è½½ (ä¸­å›½å¤§é™†åœ°åŒº)
* [x] Metal/macOSå¹³å°ä¸Šä¸‹æ–‡ç¼“å­˜
* [x] FP8 KV Cache (CUDA)
* [x] FP8 KV Cache (Metal)
* [ ] FP8 KV Cache (with Flash-Attn)
* [ ] æ”¯æŒæ›´å¤šæ¨¡å‹ç±»å‹ï¼ˆGLM 4.6, Kimi K2 Thinkingç­‰ï¼‰
* [x] CPU KV Cache å¸è½½
* [x] PDï¼ˆPrefill/Decodeï¼‰åˆ†ç¦»ï¼ˆCUDAï¼‰
* [x] PDï¼ˆPrefill/Decodeï¼‰åˆ†ç¦»ï¼ˆMetalï¼‰
* [x] å†…ç½® ChatGPTé£æ ¼ Web ç½‘é¡µæœåŠ¡
* [x] **Embedding API**
* [x] **Tokenize/Detokenize API**
* [x] **MCPé›†æˆä¸å·¥å…·è°ƒç”¨**

## ğŸ“š å‚è€ƒé¡¹ç›®

å‚è€ƒï¼š

* [Candle-vLLM](https://github.com/EricLBuehler/candle-vllm)
* Python nano-vllm é¡¹ç›®

---

ğŸ’¡ **å–œæ¬¢è¿™ä¸ªé¡¹ç›®ï¼Ÿæ¬¢è¿ â­ æ”¶è—å’Œå‚ä¸è´¡çŒ®ï¼**
