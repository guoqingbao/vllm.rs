# ğŸš€ **vLLM.rs** â€“ ç”¨ Rust å®ç°çš„æç®€ vLLM

ä¸€ä¸ªæé€Ÿ âš¡ã€è½»é‡çš„ ğŸ¦€**Rust å®ç°ç‰ˆ vLLM**ã€‚

---

<p align="center">
  <a href="./ReadMe.md">English</a> |
  <a href="./ReadMe-CN.md">ç®€ä½“ä¸­æ–‡</a> |
</p>

## âœ¨ ä¸»è¦ç‰¹æ€§

* ğŸ”§ **çº¯ Rust åç«¯** â€“ å®Œå…¨**ä¸ä¾èµ– PyTorch**
* ğŸš€ **é«˜æ€§èƒ½** (æ”¯æŒ**ä¸Šä¸‹æ–‡ç¼“å­˜**) â€“ æ€§èƒ½ä¼˜äº vLLM å’Œ Nano-vLLM
* ğŸ§  **æç®€æ ¸å¿ƒ** â€“ æ ¸å¿ƒé€»è¾‘ä»… **< 1000 è¡Œ** Rust ä»£ç 
* ğŸ’» **è·¨å¹³å°æ”¯æŒ** â€“ æ”¯æŒ **CUDA**ï¼ˆLinux/Windowsï¼‰ä¸ **Metal**ï¼ˆmacOSï¼‰
* ğŸ¤– **å†…ç½®èŠå¤©/API æœåŠ¡** â€“ Rust åŸç”Ÿå®ç°çš„èŠå¤©ä¸ API æœåŠ¡
* ğŸ **è½»é‡ Python æ¥å£** â€“ ä½¿ç”¨ PyO3 æ„å»ºçš„ Python èŠå¤©æ¥å£
* ğŸ¤ **æ¬¢è¿è´¡çŒ®** â€“ æ¬¢è¿æäº¤ PRã€é—®é¢˜æˆ–ç»™é¡¹ç›®ç‚¹äº® â­ï¼

---
### å¯¹è¯æ€§èƒ½

> A100 (å•å¡, 40G)

| æ¨¡å‹ | æ ¼å¼ | å¤§å° | è¾“å‡ºé€Ÿåº¦ |
|------------------|---------------|----------|------------------------|
| Llama-3.1-8B | ISQ (BF16->Q4K) | 8B | **90.19** tokens/s |
| DeepSeek-R1-Distill-Llama-8B | Q2_K | 8B | **94.47** tokens/s |
| DeepSeek-R1-0528-Qwen3-8B | Q4_K_M | 8B | **82.14** tokens/s |
| GLM-4-9B-0414 | Q4_K_M | 9B | **70.38** tokens/s |
| QwQ-32B | Q4_K_M | 32B | **35.69** tokens/s |
| **Qwen3-30B-A3B** | Q4_K_M | **30B (MoE)** | **75.91** tokens/s  |

### æ€§èƒ½å¯¹æ¯”

> æ¨¡å‹: Qwen3-0.6B (BF16)ï¼›
> å¹¶å‘è¯·æ±‚æ•°: 256ï¼›
> Max Model Length: 1024ï¼›
> æ¯ä¸ªè¯·æ±‚æœ€å¤§è¾“å‡º: 1024

| æ¨ç†å¼•æ“ | Tokens | è€—æ—¶ (s) | ååç‡ (tokens/s) |
|------------------|---------------|----------|------------------------|
| vLLM (RTX 4070) (Reference)          | 133,966       | 98.37    | 1361.84                |
| Nano-vLLM (RTX 4070) (Reference)      | 133,966       | 93.41    | 1434.13                |
| **vLLM.rs** (**A100**)        | 262,144       | 23.88s    | **10977.55** (**æå‡40%+**)               |
| Nano-vLLM (A100)       | 262,144       | 34.22s    |   7660.26      | 

#### å¤ç°æ­¥éª¤

**vLLM.rs**
```shell
pip install vllm_rs
python -m vllm_rs.completion --w /home/Qwen3-0.6B/ --batch 256 --max-tokens 1024 --max-model-len 1024

# æ—¥å¿—è¾“å‡º
Allocating 8192 KV blocks (28672 MB) for [256 seqs x 1024 tokens]
Maximum batched tokens 262144 (8192 blocks x Block_Size 32 for KV cache).
Start inference with 256 prompts
--- Performance Metrics ---
â±ï¸ Prompt tokens: 4096 in 0.28s (14894.55 tokens/s)
â±ï¸ Decoded tokens: 258048 in 23.60s (10944.62 tokens/s)
```

**Nano-vLLM** 

   ğŸ’¡ ä¸ºå…¬å¹³æ¯”è¾ƒï¼Œè¯·ä¿®æ”¹æ‰€æœ‰è¯·æ±‚æœ€é•¿è¾“å‡ºä¸ºå›ºå®šå€¼ï¼ˆå¦‚1024ï¼‰ï¼Œè€Œééšæœºå€¼ï¼ˆ100-1024)
```shell
pip install git+https://github.com/GeeeekExplorer/nano-vllm.git
python3 bench.py
# æ—¥å¿—è¾“å‡º
Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.65s/it, Prefill=1tok/s, Decode=369tok/s]
Total: 262144tok, Time: 34.22s, Throughput: 7660.26tok/s
```

### vLLM.rs åœ¨ **Metal (Apple Silicon, M4)** ä¸Šçš„æ€§èƒ½
> æ¨¡å‹: Qwen3-0.6B (BF16), Qwen3-4B (Q4_K_M), Qwen3-8B (Q2_K)ï¼›
> å¹¶å‘è¯·æ±‚æ•°: 1 - 128ï¼›
> Max Model Length: 512 - 2048ï¼›
> æ¯ä¸ªè¯·æ±‚æœ€å¤§è¾“å‡º: 512 - 2048ï¼›

| æ¨¡å‹ | å¹¶å‘æ•° | è¾“å‡ºTokens | è€—æ—¶ (s) | ååé‡ (tokens/s) |
|------------------|--------|--------|---------|-------------|
| Qwen3-0.6B (BF16) |  128  | 63488       | 83.13s    | 763.73     |
| Qwen3-0.6B (BF16) |  32      | 15872       | 23.53s    | 674.43    |
| Qwen3-0.6B (BF16) | 1       | 456       | 9.23s    | 49.42       |
| Qwen3-4B (Q4_K_M)  | 1       | 1683       | 52.62s    | 31.98     |
| Qwen3-8B (Q2_K)  | 1       | 1300       | 80.88s    | 16.07     |

## ğŸ§  æ”¯æŒçš„æ¨¡å‹æ¶æ„

* âœ… LLaMa ç³»åˆ—ï¼ˆLLaMa2ã€LLaMa3ï¼‰
* âœ… Qwen ç³»åˆ—ï¼ˆQwen2ã€Qwen3ï¼‰
* âœ… Qwen3 MoE ç³»åˆ—
* âœ… Mistral
* âœ… GLM4 (0414ç‰ˆæœ¬, **éChatGLM**)

æ”¯æŒ **Safetensor** å’Œ **GGUF** æ ¼å¼ã€‚

## ğŸ“¦ ä»pipå®‰è£…

```shell
# é»˜è®¤æ”¯æŒä¸Šä¸‹æ–‡ç¼“å­˜ï¼ˆå¿«é€Ÿå“åº”åŠŸèƒ½ï¼‰
python3 -m pip install vllm_rs
```


## ğŸ“˜ ä½¿ç”¨æ–¹æ³•ï¼ˆPythonï¼‰

### ğŸŒâœ¨ API Server
   ğŸ’¡ä½ å¯ä»¥ä½¿ç”¨**ä»»ä½•å…¼å®¹ OpenAI API çš„å®¢æˆ·ç«¯**è¿›è¡Œäº¤äº’ã€‚

```bash
# å®‰è£…web serviceä¾èµ–
pip install fastapi uvicorn
# å¯åŠ¨ OpenAI å…¼å®¹çš„ API æœåŠ¡ï¼ˆç›‘å¬ http://0.0.0.0:8000ï¼‰
# openai.base_url = "http://localhost:8000/v1/"
# openai.api_key = "EMPTY"
python -m vllm_rs.server --w /path/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --host 0.0.0.0 --port 8000
# æˆ–å¤šGPUæ¨ç†
python -m vllm_rs.server --w /path/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --d 0,1 --host 0.0.0.0 --port 8000 --max-model-len 64000
# æˆ–å¤šGPUæ¨ç†ï¼ˆåŒæ—¶å°†æƒé‡é‡åŒ–ä¸ºQ4Kæ ¼å¼ï¼Œå¯ç”¨æœ€é•¿ä¸Šä¸‹æ–‡ï¼‰ï¼š
python -m vllm_rs.server --w /path/Qwen3-30B-A3B-Instruct-2507 --d 0,1 --host 0.0.0.0 --port 8000 --isq q4k --max-model-len 262144 --max-num-seqs 1

# æˆ–å¤šGPUæ¨ç†+ä¸Šä¸‹æ–‡ç¼“å­˜ (ç¼“å­˜ä¸Šä¸‹æ–‡ï¼Œé€šè¿‡OpenAI APIå‘èµ·è¯·æ±‚æ—¶åœ¨`extra_body`å­—æ®µé‡Œä¼ å…¥`session_id`ï¼Œ`session_id`åœ¨å¯¹è¯è¿‡ç¨‹ä¸­ä¿æŒä¸å˜ï¼Œæ–°å¯¹è¯éœ€è¦å¯ç”¨æ–°çš„`session_id`ï¼Œæ— éœ€æ”¹å˜å…¶å®ƒè®¾ç½®)
python -m vllm_rs.server --w /path/Qwen3-30B-A3B-Instruct-2507 --d 0,1 --host 0.0.0.0 --port 8000 --isq q4k --max-model-len 64000 --max-num-seqs 8 
```

### ğŸ¤–âœ¨ äº¤äº’å¼èŠå¤©ä¸æ‰¹å¤„ç†

```bash
# äº¤äº’å¼èŠå¤©
python -m vllm_rs.chat --i --w /path/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf

# æŒ‡å®šè®¾å¤‡2 (è®¾å¤‡åºå·ä¸º1ï¼Œ`--d 1`)
python -m vllm_rs.chat --i --d 1 --w /path/GLM-4-9B-0414-Q4_K_M.gguf

# å°†æœªé‡åŒ–æ¨¡å‹åŠ è½½ä¸ºGGUFé‡åŒ–æ¨¡å‹ (ä¾‹å¦‚q4kæ ¼å¼)ï¼Œå¹¶å¯ç”¨æœ€é•¿ä¸Šä¸‹æ–‡ï¼ˆ262144 tokensï¼‰ï¼Œé€‚ç”¨äºä»»æ„å·²æ”¯æŒçš„æ¨¡å‹æ¶æ„
python -m vllm_rs.chat --i --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --max-model-len 262144 --max-num-seqs 1

# å¯ç”¨ä¸Šä¸‹æ–‡ç¼“å­˜ï¼ˆå¿«é€Ÿå“åº”è¯·æ±‚ï¼‰
python -m vllm_rs.chat --i --d 0 --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --max-model-len 262144 --max-num-seqs 1 --context-cache

# æ‰¹é‡åŒæ­¥ç¤ºä¾‹
python -m vllm_rs.completion --w /path/qwq-32b-q4_k_m.gguf --d 0,1 --prompts "How are you? | How to make money?"

# æ‰¹é‡åŒæ­¥ç¤ºä¾‹ (å¤šGPU)
python -m vllm_rs.completion --w /home/GLM-4-9B-0414 --d 0,1 --batch 8 --max-model-len 1024 --max-tokens 1024
```

### ğŸ Python API
```python
from vllm_rs import Engine, EngineConfig, SamplingParams, Message
cfg = EngineConfig(model_path="/path/Qwen3-8B-Q2_K.gguf", max_model_len=4096)
engine = Engine(cfg, "bf16")
params = SamplingParams(temperature=0.6, max_tokens=256)
prompt = engine.apply_chat_template([Message("user", "How are you?")], True)

# åŒæ­¥æ‰¹é‡ç”Ÿæˆ
outputs = engine.generate_sync([params,params], [prompt, prompt])
print(outputs)

# å•è¯·æ±‚æµå¼ç”Ÿæˆ
stream = engine.generate_stream(params, prompt)
for token in stream:
    print(token)
```

## ğŸ”¨ ä»æºä»£ç ç¼–è¯‘å®‰è£…ï¼ˆå¯é€‰ï¼‰

> âš ï¸ å¯ç”¨ Flash Attentionï¼ˆCUDAï¼‰æ—¶ï¼Œé¦–æ¬¡ç¼–è¯‘å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ã€‚

### ğŸ› ï¸ ç¯å¢ƒè¦æ±‚

* å®‰è£… [Rust å·¥å…·é“¾](https://www.rust-lang.org/tools/install)
* **macOS** å¹³å°éœ€å®‰è£… [Xcode å‘½ä»¤è¡Œå·¥å…·](https://mac.install.guide/commandlinetools/)
* æ„å»º Python æ¥å£éœ€å®‰è£… [Maturin](https://github.com/PyO3/maturin)

### ç¼–è¯‘æ­¥éª¤
1. **å®‰è£… Maturin**

```bash
sudo apt install libssl-dev pkg-config -y # ç¼–è¯‘ä¾èµ– (Linux)
pip install maturin
pip install maturin[patchelf]  # Linux/Windows å¹³å°
```

2. **æ„å»º Python åŒ…**

```bash
# CUDAï¼ˆè¾ƒçŸ­ä¸Šä¸‹æ–‡ï¼‰
maturin build --release --features cuda,python

# CUDA + Flash Attention (è¶…é•¿ä¸Šä¸‹æ–‡ (>32kæ—¶) æ¨èå¯ç”¨ï¼‰
maturin build --release --features cuda,flash-attn,python

# macOSï¼ˆMetalï¼‰
maturin build --release --features metal,python

# å¤šGPUæ¨ç† (CUDA, ç”Ÿæˆç‹¬ç«‹çš„runnerï¼Œè¿è¡Œäºä¸åŒè¿›ç¨‹)
./build.sh --release --features cuda,nccl,flash-attn,python

# å¤šGPUæ¨ç† + ä¸Šä¸‹æ–‡ç¼“å­˜
./build.sh --release --features cuda,nccl,flash-decoding,python
```

3. **å®‰è£…æ„å»ºå¥½çš„åŒ…ä¸ä¾èµ–**

```bash
pip install target/wheels/vllm_rs-*-cp38-abi3-*.whl --force-reinstall
pip install fastapi uvicorn
```

## ğŸ“˜ ä½¿ç”¨æ–¹æ³•ï¼ˆRustï¼‰
### ğŸ¤–âœ¨ Rust CLI æ¨¡å¼

ä½¿ç”¨ `--i` å¯ç”¨äº¤äº’æ¨¡å¼ï¼Œ`--w` æŒ‡å®šæ¨¡å‹è·¯å¾„ï¼š

```bash
# CUDAï¼ˆçŸ­ä¸Šä¸‹æ–‡ï¼‰
cargo run --release --features cuda -- --i --w /path/qwq-32b-q4_k_m.gguf

# ä½¿ç”¨ç¬¬ä¸‰ä¸ªè®¾å¤‡ (è®¾å¤‡åºå·2ï¼Œ`--d 2`)
cargo run --release --features cuda -- --i --d 2 --w /path/GLM-4-9B-0414-Q4_K_M.gguf

# CUDA + Flash Attentionï¼ˆè¶…é•¿ä¸Šä¸‹æ–‡ï¼Œå¦‚ 256k tokensï¼‰
cargo run --release --features cuda,nccl,flash-attn -- --i --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --max-model-len 262144

# CUDA + Context Cache
cargo run --release --features cuda,nccl,flash-decoding -- --i --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --max-model-len 262144 --context-cache

# macOSï¼ˆMetalï¼‰
cargo run --release --features metal -- --i --w /path/DeepSeek-R1-Distill-Llama-8B-Q2_K.gguf
```

Safetensor æ¨¡å‹ï¼ˆæœªé‡åŒ–ï¼‰

```bash
# CUDA
cargo run --release --features cuda,flash-attn -- --w /path/Qwen3-8B/ --prompts "How are you today?"

# Metalï¼ˆmacOSï¼‰, å¤šä¸ª prompt ä½¿ç”¨ `|` åˆ†éš”
cargo run --release --features metal -- --w /path/Qwen3-8B/ --prompts "Talk about China. | Talk about America."

# å¤šGPUæ¨ç†ï¼ˆäº¤äº’æ¨¡å¼ï¼‰
./run.sh --release --features cuda,nccl -- --w /home/GLM-4-9B-0414 --d 0,1 --i --max-tokens 1024 --max-model-len 1024

# å¤šGPUæ¨ç†+ä¸Šä¸‹æ–‡ç¼“å­˜ï¼ˆäº¤äº’æ¨¡å¼ï¼‰
./run.sh --release --features cuda,nccl,flash-decoding -- --w /home/GLM-4-9B-0414 --d 0,1 --i --max-tokens 1024 --max-model-len 1024 --context-cache
```

### âš™ï¸ å‘½ä»¤è¡Œå‚æ•°è¯´æ˜

| å‚æ•°          | æè¿°                                     |       |
| ----------- | -------------------------------------- | ----- |
| `--w`       | æ¨¡å‹è·¯å¾„ï¼ˆSafetensor ç›®å½•æˆ– GGUF æ–‡ä»¶ï¼‰           |       |
| `--d`       | è®¾å¤‡ IDï¼Œä¾‹å¦‚ `--d 0`                       |       |
| `--max-num-seqs`   | åŒæ—¶å¤„ç†çš„æœ€å¤§è¯·æ±‚æ•°ï¼ˆé»˜è®¤ `32`, macOSå¹³å°ä¸º`8`ï¼‰   |       |
| `--max-tokens`     | å•æ¬¡æœ€å¤§è¾“å‡º token æ•°ï¼ˆé»˜è®¤ `4096`ï¼Œä¸Šé™ä¸ºæ¨¡å‹æ”¯æŒçš„æœ€å¤§é•¿åº¦ï¼‰ |       |
| `--batch`     | ä»…ç”¨äºæ€§èƒ½ (å¯ç”¨åä¼šå¿½ç•¥ `max-num-seqs` ä¸ `prompts`) |    |
| `--prompts` | è¾“å…¥çš„ promptï¼Œå¤šä¸ªä½¿ç”¨ \| åˆ†éš” |
| `--dtype`   | KV ç¼“å­˜æ•°æ®ç±»å‹ï¼š`bf16`ï¼ˆé»˜è®¤ï¼‰ã€`f16` æˆ– `f32`     |       |
| `--isq`   | å°†æœªé‡åŒ–æ¨¡å‹åŠ è½½ä¸ºGGUFé‡åŒ–æ¨¡å‹ï¼Œå¯é€‰`q2k`, `q4k`  ç­‰   |       |
| `--temperature`   | é‡‡æ ·æ¸©åº¦ (sampling temperature)ï¼Œæ§åˆ¶è¾“å‡ºâ€œéšæœºæ€§/åˆ›é€ æ€§â€çš„ä¸€ä¸ªè¶…å‚æ•°ï¼Œä»‹äº0-1ä¹‹é—´  |       |
| `--top-k`   | top-k æ§åˆ¶æ¨¡å‹åœ¨æ¯ä¸€æ­¥åªä»å‰ k ä¸ªæœ€é«˜æ¦‚ç‡çš„è¯é‡ŒæŒ‘é€‰ï¼Œk è¶Šå° â†’ è¶Šç¨³å®šï¼›k è¶Šå¤§ â†’ è¶Šéšæœº   |       |
| `--top-p`   | top-p é‡‡æ ·æ ¹æ®æ¦‚ç‡é˜ˆå€¼é€‰æ‹©åŠ¨æ€æ•°é‡çš„å€™é€‰ï¼ŒèŒƒå›´æ˜¯ [0,1]ï¼Œå¸¸ç”¨åœ¨ 0.8 ~ 0.95   |       |
| `--penalty`   | repetition Penalty æ§åˆ¶æ¨¡å‹é¿å…é‡å¤ï¼Œå€¼ä¸º â‰¥ 1.0ï¼Œé€šå¸¸å– 1.1 ~ 2.0ã€‚æ•°å€¼è¶Šå¤§ï¼Œæƒ©ç½šè¶Šå¼ºï¼Œè¾“å‡ºè¶Šå¤šæ ·åŒ–   |       |

## ğŸ“½ï¸ æ¼”ç¤ºè§†é¢‘

ğŸ‰ è§‚çœ‹é¡¹ç›®è¿è¡Œæ¼”ç¤ºï¼š

<video src="https://github.com/user-attachments/assets/0751471b-a0c4-45d7-acc6-99a3e91e4c91" width="70%"></video>


## ğŸ—œï¸ å®æ—¶é‡åŒ–ï¼ˆGGUF æ ¼å¼è½¬æ¢ï¼‰

   ğŸ’¡ å°†ä»»æ„éé‡åŒ–æ¨¡å‹å®æ—¶é‡åŒ–åŠ è½½ä¸ºGGUFæ ¼å¼ï¼ŒæŒ‡å®š`--isq`éq4kã€q8_0æ—¶å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼š

```bash
# macOS
cargo run --release --features metal -- --w /path/Qwen3-0.6B/ --isq q4k --prompts "How are you today?"

# CUDA
cargo run --release --features cuda,flash-attn -- --w /path/Qwen3-8B/ --isq q4k --prompts "How are you today?"
```


## ğŸ“Œ é¡¹ç›®çŠ¶æ€

> ğŸš§ **é¡¹ç›®ä»åœ¨ç§¯æå¼€å‘ä¸­ï¼Œæ¥å£ä¸åŠŸèƒ½å¯èƒ½å‘ç”Ÿå˜æ›´ã€‚**

## ğŸ› ï¸ å¼€å‘è®¡åˆ’ï¼ˆTODOï¼‰

* [x] Metal å¹³å°æ”¯æŒæ‰¹é‡æ¨ç†
* [x] æ”¯æŒ GGUF æ ¼å¼
* [x] CUDA å¹³å° Flash Attention æ”¯æŒ
* [x] CUDA Graph
* [x] OpenAI API å…¼å®¹æœåŠ¡å™¨ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
* [x] æŒç»­æ‰¹å¤„ç†
* [x] å¤šå¡å¹¶è¡Œæ¨ç†
* [x] Metal/macOSå¹³å°Promptå¤„ç†åŠ é€Ÿ
* [x] åˆ†å—é¢„å¡«å……ï¼ˆChunked Prefillï¼‰
* [x] ä¸Šä¸‹æ–‡ç¼“å­˜ (å½“`flash-decoding`ç‰¹æ€§å¯ç”¨æ—¶ç”Ÿæ•ˆ)
* [ ] æ”¯æŒæ›´å¤šæ¨¡å‹ç±»å‹


## ğŸ“š å‚è€ƒé¡¹ç›®

å‚è€ƒï¼š

* [Candle-vLLM](https://github.com/EricLBuehler/candle-vllm)
* Python nano-vllm é¡¹ç›®

---

ğŸ’¡ **å–œæ¬¢è¿™ä¸ªé¡¹ç›®ï¼Ÿæ¬¢è¿ â­ æ”¶è—å’Œå‚ä¸è´¡çŒ®ï¼**
