# ğŸš€ **vLLM.rs** â€“ ç”¨ Rust å®ç°çš„æç®€ vLLM

ä¸€ä¸ªæé€Ÿ âš¡ã€è½»é‡çš„ ğŸ¦€**Rust å®ç°ç‰ˆ vLLM**ã€‚

---

<p align="center">
  <a href="./ReadMe.md">English</a> |
  <a href="./ReadMe-CN.md">ç®€ä½“ä¸­æ–‡</a> |
</p>

## âœ¨ ä¸»è¦ç‰¹æ€§

* ğŸ”§ **çº¯ Rust åç«¯** â€“ å®Œå…¨**ä¸ä¾èµ– PyTorch**
* ğŸš€ **é«˜æ€§èƒ½** (æ”¯æŒ**ä¸Šä¸‹æ–‡ç¼“å­˜ã€PDåˆ†ç¦»**) â€“ æ€§èƒ½ä¼˜äºPythonåŒç±»æ¨ç†æ¡†æ¶
* ğŸ§  **æç®€æ ¸å¿ƒ** â€“ æ ¸å¿ƒé€»è¾‘ä»… **<3000 è¡Œ** Rust ä»£ç 
* ğŸ’» **è·¨å¹³å°æ”¯æŒ** â€“ æ”¯æŒ **CUDA**ï¼ˆLinux/Windowsï¼‰ä¸ **Metal**ï¼ˆmacOSï¼‰
* ğŸ¤– **å†…ç½®èŠå¤©/API æœåŠ¡** â€“ Rust åŸç”Ÿå®ç°çš„èŠå¤©ä¸ API æœåŠ¡
* ğŸ **è½»é‡ Python æ¥å£** â€“ ä½¿ç”¨ PyO3 æ„å»ºçš„ Python èŠå¤©æ¥å£
* ğŸ¤ **æ¬¢è¿è´¡çŒ®** â€“ æ¬¢è¿æäº¤ PRã€é—®é¢˜æˆ–ç»™é¡¹ç›®ç‚¹äº® â­ï¼

---
### ğŸ’¬ å¯¹è¯æ€§èƒ½

> **A100** (å•å¡, 40G)

| æ¨¡å‹ | æ ¼å¼ | å¤§å° | è¾“å‡ºé€Ÿåº¦ |
|------------------|---------------|----------|------------------------|
| Llama-3.1-8B | ISQ (BF16->Q4K) | 8B | **90.19** tokens/s |
| DeepSeek-R1-Distill-Llama-8B | Q2_K | 8B | **94.47** tokens/s |
| DeepSeek-R1-0528-Qwen3-8B | Q4_K_M | 8B | **95** tokens/s |
| GLM-4-9B-0414 | Q4_K_M | 9B | **70.38** tokens/s |
| QwQ-32B | Q4_K_M | 32B | **35.69** tokens/s |
| **Qwen3-30B-A3B** | Q4_K_M | **30B (MoE)** | **75.91** tokens/s  |

> vLLM.rs åœ¨ **Metal (Apple Silicon, M4)** ä¸Šçš„æ€§èƒ½
  <details>
    <summary>å±•å¼€è¯¦æƒ…</summary>

   > æ¨¡å‹: Qwen3-0.6B (BF16), Qwen3-4B (Q4_K_M), Qwen3-8B (Q2_K)ï¼›
   > å¹¶å‘è¯·æ±‚æ•°: 1 - 128ï¼›
   > Max Model Length: 512 - 2048ï¼›
   > æ¯ä¸ªè¯·æ±‚æœ€å¤§è¾“å‡º: 512 - 2048ï¼›

   | æ¨¡å‹ | å¹¶å‘æ•° | è¾“å‡ºTokens | è€—æ—¶ (s) | ååé‡ (tokens/s) |
   |------------------|--------|--------|---------|-------------|
   | Qwen3-0.6B (BF16) |  128  | 63488       | 83.13s    | 763.73     |
   | Qwen3-0.6B (BF16) |  32      | 15872       | 23.53s    | 674.43    |
   | Qwen3-0.6B (BF16) | 1       | 456       | 9.23s    | 49.42       |
   | Qwen3-4B (Q4_K_M)  | 1       | 1683       | 52.62s    | 31.98     |
   | Qwen3-8B (Q2_K)  | 1       | 1300       | 80.88s    | 16.07     |

  </details>

### æ€§èƒ½å¯¹æ¯”

> æ¨¡å‹: Qwen3-0.6B (BF16)ï¼›
> å¹¶å‘è¯·æ±‚æ•°: 256ï¼›
> Max Model Length: 1024ï¼›
> æ¯ä¸ªè¯·æ±‚æœ€å¤§è¾“å‡º: 1024

| æ¨ç†å¼•æ“ | Tokens | è€—æ—¶ (s) | ååç‡ (tokens/s) |
|------------------|---------------|----------|------------------------|
| vLLM (RTX 4070) (Reference)          | 133,966       | 98.37    | 1361.84                |
| Nano-vLLM (RTX 4070) (Reference)      | 133,966       | 93.41    | 1434.13                |
| **vLLM.rs** (**A100**)        | 262,144       | 23.88s    | **10977.55** (**æå‡40%+**)               |
| Nano-vLLM (A100)       | 262,144       | 34.22s    |   7660.26      | 

<a href="python/ReadMe.md">å¤ç°æ­¥éª¤</a>


## ğŸ§  æ”¯æŒçš„æ¨¡å‹æ¶æ„

* âœ… LLaMa ç³»åˆ—ï¼ˆLLaMa2ã€LLaMa3ï¼‰
* âœ… Qwen ç³»åˆ—ï¼ˆQwen2ã€Qwen3ï¼‰
* âœ… Qwen2 Moe ç³»åˆ—ï¼ˆä½¿ç”¨Qwen3 MoEæµç¨‹+å…±äº«ä¸“å®¶å±‚ï¼‰
* âœ… Qwen3 MoE ç³»åˆ—
* âœ… Mistral
* âœ… GLM4 (0414ç‰ˆæœ¬, **éChatGLM**)

æ”¯æŒ **Safetensor** (åŒ…å«GPTQ, AWQé‡åŒ–æ ¼å¼) å’Œ **GGUF** æ ¼å¼ã€‚


## ğŸ“˜ ä½¿ç”¨æ–¹æ³•ï¼ˆPythonï¼‰
### ğŸ“¦ ä»pipå®‰è£…
   ğŸ’¡ 1. CUDA compute capability < 8.0 GPUè®¾å¤‡ï¼ˆä¾‹å¦‚V100ï¼Œä¸æ”¯æŒflash-attnç‰¹æ€§ï¼‰ä¸Šéœ€è¦æ‰‹åŠ¨ç¼–è¯‘å®‰è£…
   
   ğŸ’¡ 2. é¢„ç¼–è¯‘åŒ…`context cache` ä¾èµ–äºFlash attention, å¦‚éœ€FP8 KvCacheï¼Œè¯·é‡æ–°ç¼–è¯‘å¹¶å»é™¤`flash-context`ç‰¹æ€§
```shell
# å¤šå¡éœ€è¦å®‰è£…NCCLåº“
python3 -m pip install vllm_rs fastapi uvicorn
```

### ğŸŒâœ¨ API Server
   ğŸ’¡ä½ å¯ä»¥ä½¿ç”¨**ä»»ä½•å…¼å®¹ OpenAI API çš„å®¢æˆ·ç«¯**è¿›è¡Œäº¤äº’ã€‚

   ğŸ’¡å¦‚æ–°çš„é•¿æ–‡æœ¬è¯·æ±‚å¯¼è‡´å½“å‰ç”Ÿæˆè¿‡ç¨‹å¡é¡¿ï¼Œè¯·ä½¿ç”¨ **Rust PD Server/Client** ï¼ˆè§**PDåˆ†ç¦»**ï¼‰

   ğŸ¤– <a href="python/ReadMe.md">è¿™é‡ŒåŒ…å«å®¢æˆ·ç«¯ä½¿ç”¨Context-cacheçš„æ³¨æ„äº‹é¡¹</a>

  <details open>
    <summary>å•å¡ + GGUFæ¨¡å‹</summary>

   ```bash
   # å®¢æˆ·ç«¯é»˜è®¤é…ç½®ï¼ˆå¦‚å®¢æˆ·ç«¯ä¸API Serveråœ¨åŒä¸€ç³»ç»Ÿï¼‰ï¼š
   # openai.base_url = "http://localhost:8000/v1/"
   # openai.api_key = "EMPTY"

   # `--m`: model_id, `--f`: GGUFæ–‡ä»¶å
   python3 -m vllm_rs.server --m unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF --f Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf
   ```
  </details>

   <details open>
    <summary>å¤šGPU + æœ¬åœ°GGUFæ¨¡å‹</summary>

   ```bash
   python3 -m vllm_rs.server --f /path/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --d 0,1 --max-model-len 64000
   ```
  </details>

   <details open>
    <summary>å°†æœªé‡åŒ–æ¨¡å‹åŠ è½½ä¸ºGGUFæ¨¡å‹</summary>

   ```bash
   # åŒæ—¶å°†æƒé‡é‡åŒ–ä¸ºQ4Kæ ¼å¼ï¼Œå¯ç”¨æœ€é•¿ä¸Šä¸‹æ–‡ï¼š
   python3 -m vllm_rs.server --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --d 0,1 --host 0.0.0.0 --port 8000 --max-model-len 262144 --max-num-seqs 1
   ```
  </details>

  <details>
    <summary>è¿è¡ŒGPTQ/AWQ Marlinå…¼å®¹æ¨¡å‹</summary>

```bash
python3 -m vllm_rs.server --w /home/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4-Marlin
```

  </details>

   <details>
    <summary>å¤šGPU + GGUFæ¨¡å‹ + ä¸Šä¸‹æ–‡ç¼“å­˜</summary>
   
   ```bash
   # ç¼“å­˜ä¸Šä¸‹æ–‡å¯ç”¨æ—¶ï¼Œé€šè¿‡OpenAI APIå‘èµ·è¯·æ±‚æ—¶åœ¨`extra_body`å­—æ®µé‡Œä¼ å…¥`session_id`ï¼Œ`session_id`åœ¨å¯¹è¯è¿‡ç¨‹ä¸­ä¿æŒä¸å˜ï¼Œæ–°å¯¹è¯éœ€è¦å¯ç”¨æ–°çš„`session_id`ï¼Œæ— éœ€æ”¹å˜å…¶å®ƒè®¾ç½®
   python3 -m vllm_rs.server --m unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF --f Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --d 0,1 --host 0.0.0.0 --port 8000 --max-model-len 64000 --max-num-seqs 8 --context-cache
   ```
  </details>

### ğŸ¤–âœ¨ äº¤äº’å¼èŠå¤©ä¸æ‰¹å¤„ç†

  <details open>
    <summary>ä½¿ç”¨Huggingface æ¨¡å‹idåŠ è½½</summary>

   ```bash
   # é»˜è®¤ä½¿ç”¨Context-cache
   python3 -m vllm_rs.chat --m unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF --f Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf
   ```
  </details>

  <details open>
    <summary>å°†æœªé‡åŒ–æ¨¡å‹åŠ è½½ä¸ºGGUFé‡åŒ–æ¨¡å‹</summary>

   ```bash
   # å¹¶å¯ç”¨æœ€é•¿ä¸Šä¸‹æ–‡ï¼ˆ262144 tokensï¼‰
   python3 -m vllm_rs.chat --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --max-model-len 262144
   ```
  </details>

  <details>
    <summary>æ‰¹é‡åŒæ­¥ç¤ºä¾‹</summary>

   ```bash
   python3 -m vllm_rs.completion --f /path/qwq-32b-q4_k_m.gguf --d 0,1 --prompts "How are you? | How to make money?"
   ```

   ```bash
   python3 -m vllm_rs.completion --w /home/GLM-4-9B-0414 --d 0,1 --batch 8 --max-model-len 1024 --max-tokens 1024
   ```
  </details>


#### ğŸ Python API
  <details>
    <summary>è¯¦æƒ…</summary>

   ```python
   from vllm_rs import Engine, EngineConfig, SamplingParams, Message
   cfg = EngineConfig(weight_path="/path/Qwen3-8B-Q2_K.gguf", max_model_len=4096)
   engine = Engine(cfg, "bf16")
   params = SamplingParams(temperature=0.6, max_tokens=256)
   prompt = engine.apply_chat_template([Message("user", "How are you?")], True)

   åŒæ­¥æ‰¹é‡ç”Ÿæˆ
   outputs = engine.generate_sync([params,params], [prompt, prompt])
   print(outputs)

   params.session_id = xxx #ä¼ å…¥session_idä»¥ä½¿ç”¨ä¸Šä¸‹æ–‡ç¼“å­˜åŠŸèƒ½

   å•è¯·æ±‚æµå¼ç”Ÿæˆ
   (seq_id, prompt_length, stream) = engine.generate_stream(params, prompt)
   for item in stream:
      # item.datatype == "TOKEN"
      print(item.data)
   ```
  </details>

## ğŸ“˜ ä½¿ç”¨æ–¹æ³•ï¼ˆRustï¼‰

ä½¿ç”¨ `--i` å¯ç”¨äº¤äº’æ¨¡å¼ ğŸ¤–ï¼Œ`--server` å¯ç”¨æœåŠ¡æ¨¡å¼ ğŸŒï¼Œ`--m`æŒ‡å®šHuggingfaceæ¨¡å‹ï¼Œæˆ–`--w` æŒ‡å®šæœ¬åœ°Safetensorsæ¨¡å‹è·¯å¾„ æˆ–`--f` æŒ‡å®šGGUFæ¨¡å‹æ–‡ä»¶ï¼š

> Chatæ¨¡å¼
  <details open>
    <summary>å•å¡æ¨ç† + å†…ç½®Context Cache</summary>

   ```bash
   cargo run --release --features cuda -- --i --m unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF --f Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf
   ```
  </details>

  <details open>
    <summary>å¤šå¡æ¨ç† + é«˜æ€§èƒ½Context Cache</summary>

   ```bash
   # éœ€ä½¿ç”¨run.shç”Ÿæˆç‹¬ç«‹runnerï¼Œå¯ç”¨flash-contextç‰¹æ€§éœ€è¦Ampere+ä»¥ä¸Šè®¾å¤‡ï¼Œç¼–è¯‘æ—¶é—´è¾ƒé•¿
   ./run.sh --release --features cuda,nccl,flash-context --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --i
   ```
  </details>

> å¤šå¡æ¨ç† server æœåŠ¡

  <details open>
    <summary>è¿è¡Œæœªé‡åŒ–Qwen3-30B-A3Bæ¨¡å‹</summary>

   ```bash
   ./run.sh --release --features cuda,nccl,graph,flash-context --d 0,1,2,3 --w /path/Qwen3-30B-A3B-Instruct-2507 --max-model-len 100000 --max-num-seqs 4 --server --port 8000
   ```
  </details>

   <details open>
    <summary>å¤šå¡è¿è¡ŒQwen3-30B-A3Bé‡åŒ–æ¨¡å‹</summary>

   ```bash
   ./run.sh --release --features cuda,nccl,graph,flash-attn --server --d 0,1 --f /path/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf --max-model-len 262144 --context-cache
   ```
  </details>

   <details>
    <summary>å°†æœªé‡åŒ–Qwen3-30B-A3Bæ¨¡å‹è¿è¡Œä¸ºQ4Ké‡åŒ–æ¨¡å‹ï¼ŒåŒæ—¶ä½¿ç”¨FP8 KVCache</summary>

   ```bash
   # å»é™¤`flash-context`ä»¥ä½¿ç”¨fp8 kvcache
   ./run.sh --release --features cuda,nccl,flash-attn --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --max-model-len 100000 --max-num-seqs 4 --server --port 8000 --fp8-kvcache
   ```
  </details>

   <details>
    <summary>è¿›ä¸€æ­¥ä½¿ç”¨Context-CacheåŠŸèƒ½</summary>

   ä½¿ç”¨å†…ç½®Context-cacheï¼Œä¸ä¾èµ–Flash Attentionï¼Œæ”¯æŒV100, Metalå¹³å°
   ```bash
   ./run.sh --release --features cuda,nccl --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --max-model-len 100000 --max-num-seqs 4 --server --port 8000 --context-cache
   ```

   ä½¿ç”¨Flash Attentionåšcontext-cacheåŠdecodingï¼ˆéœ€è¦Ampere+ç¡¬ä»¶ï¼Œç¼–è¯‘è€—æ—¶æ—¶é•¿ï¼Œé•¿æ–‡æœ¬Prefillæ€§èƒ½æœ€é«˜ï¼‰
   ```bash
   ./run.sh --release --features cuda,nccl,flash-attn,flash-context --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --max-model-len 100000 --max-num-seqs 4 --server --port 8000 --context-cache
   ```
  </details>

> MacOS/Metalå¹³å°

  <details open>
    <summary>è¿è¡ŒQ2Ké‡åŒ–æ¨¡å‹</summary>

   ```bash
   cargo run --release --features metal -- --server --f /path/DeepSeek-R1-Distill-Llama-8B-Q2_K.gguf
   ```
  </details>

  <details>
    <summary>å°†æœªé‡åŒ–æ¨¡å‹è¿è¡Œä¸ºQ6Ké‡åŒ–æ¨¡å‹ï¼ŒåŒæ—¶ä½¿ç”¨Context-cache</summary>

   ```bash
   cargo run --release --features metal -- --i --w /path/Qwen3-0.6B --isq q6k
   ```
  </details>


## ğŸ”€ Prefill-decode åˆ†ç¦»ï¼ˆPDåˆ†ç¦»ï¼‰

  <details>
    <summary>å¯åŠ¨PDæœåŠ¡å™¨</summary>

   æ— éœ€æŒ‡å®š`port`ï¼Œå› ä¸ºæ­¤æœåŠ¡å™¨ä¸ç›´æ¥æ¥æ”¶ç”¨æˆ·è¯·æ±‚ï¼ŒKvCacheå¤§å°ç”±`--max-model-len`å’Œ`--max-num-seqs`æ§åˆ¶ã€‚
   ```bash
   # PDæœåŠ¡å™¨ä½¿ç”¨`flash-context`åŠ å¿«å¤„ç†é•¿æ–‡æœ¬prefillï¼ˆPDæœåŠ¡å™¨å¯åŠ¨éé‡åŒ–æ¨¡å‹å¯è·å¾—æœ€ä½³ååç‡ï¼‰
   ./run.sh --release --features cuda,nccl,flash-context --d 0,1 --w /path/Qwen3-30B-A3B-Instruct-2507 --max-model-len 200000 --max-num-seqs 2 --server --pd-server
   ```

   PDæœåŠ¡å™¨è¿˜å¯ä½¿ç”¨é¢„ç¼–è¯‘PythonåŒ…å¯åŠ¨ (ä¾èµ–ï¼špip install vllm_rs fastapi uvicorn)
   ```bash
   python3 -m vllm_rs.server --w /path/Qwen3-30B-A3B-Instruct-2507 --max-model-len 200000 --max-num-seqs 2 --d 0,1 --pd-server
   ```
  </details>

  <details>
    <summary>å¯åŠ¨PDå®¢æˆ·ç«¯</summary>

   PDå®¢æˆ·ç«¯å½“å‰ä»…æ”¯æŒRustç‰ˆæœ¬ï¼ŒPython PDå®¢æˆ·ç«¯ç”±äºPythonå…¨å±€é”ï¼Œä¼šå¯¼è‡´PD Serverå¤„ç†é•¿æ–‡æœ¬æ—¶å½±å“PDå®¢æˆ·ç«¯ï¼ˆå¦‚æœServer/Clientå¤„äºåŒä¸€æ“ä½œç³»ç»Ÿï¼‰
   ```bash
   ./run.sh --release --features cuda,nccl,flash-context --d 2,3 --w /path/Qwen3-30B-A3B-Instruct-2507 --isq q4k --max-model-len 200000 --max-num-seqs 2 --server --port 8000 --pd-client
   ```
  </details>

  <details>
    <summary>å•æœºå¤šä¸ªDockers/å¤šæœºé…ç½®</summary>

   PD Serverä¸Clientå¯åŠ¨æ—¶çš„æ¨¡å‹åŠRankæ•°é‡ï¼ˆå¡æ•°ï¼‰éœ€è¦ä¸€è‡´ï¼Œå¯ä¸ºç›¸åŒæ¨¡å‹çš„ä¸åŒæ ¼å¼ï¼ˆä¾‹å¦‚æœåŠ¡å™¨æœªé‡åŒ–Safetensor, å®¢æˆ·ç«¯GGUFï¼‰
   å¦‚æœæŒ‡å®šäº† `--pd-url`ï¼ˆä¾‹å¦‚ serverç«¯: 0.0.0.0:8100, clientç«¯: server_ip:8100ï¼‰ï¼ŒPD æœåŠ¡å™¨/å®¢æˆ·ç«¯å°†å°è¯•ç»‘å®šæˆ–è¿æ¥åˆ°è¯¥åœ°å€ï¼Œ
   å®¢æˆ·ç«¯å°†å°è¯•ä½¿ç”¨æŒ‡å®šçš„ URL è¿æ¥åˆ°æœåŠ¡å™¨ï¼ˆMetalå¹³å°ä¸æ”¯æŒLocalIPC, å¿…é¡»æä¾›pd-urlï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒæœåŠ¡å™¨å’Œå®¢æˆ·ç«¯å¯ä»¥éƒ¨ç½²åœ¨ä¸åŒçš„æœºå™¨ä¸Šã€‚
   å•æœºå¤šå¡ï¼ŒPDæœåŠ¡å™¨ä¸å®¢æˆ·ç«¯è¿è¡Œäºä¸åŒDockerï¼Œéœ€è¦é…ç½®Dockerå¯åŠ¨å‚æ•° `--ipc=host`
  </details>

---

## ğŸ“½ï¸ æ¼”ç¤ºè§†é¢‘

ğŸ‰ è§‚çœ‹é¡¹ç›®è¿è¡Œæ¼”ç¤ºï¼š
<video src="https://github.com/user-attachments/assets/7fc6aa0b-78ac-4323-923f-d761dd12857f" width="1000px"></video>


## ğŸ”¨ ä»æºä»£ç ç¼–è¯‘å®‰è£…ï¼ˆå¯é€‰ï¼‰

> âš ï¸ å¯ç”¨ Flash Attentionï¼ˆCUDAï¼‰æ—¶ï¼Œé¦–æ¬¡ç¼–è¯‘å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ã€‚

> âš ï¸ å¯ç”¨ ä¸Šä¸‹æ–‡ç¼“å­˜æˆ–å¤šGPUæ¨ç†æ—¶ï¼Œéœ€è¦åŒæ—¶ç¼–è¯‘`Runner`ï¼ˆä½¿ç”¨`build.sh`ç¼–è¯‘ æˆ– `run.sh`è¿è¡Œï¼‰

### ğŸ› ï¸ ç¯å¢ƒè¦æ±‚

* å®‰è£… [Rust å·¥å…·é“¾](https://www.rust-lang.org/tools/install)
* **macOS** å¹³å°éœ€å®‰è£… [Xcode å‘½ä»¤è¡Œå·¥å…·](https://mac.install.guide/commandlinetools/)
* æ„å»º Python æ¥å£éœ€å®‰è£… [Maturin](https://github.com/PyO3/maturin)

### ç¼–è¯‘æ­¥éª¤
1. **å®‰è£… Maturin**

```bash
sudo apt install libssl-dev pkg-config -y # ç¼–è¯‘ä¾èµ– (Linux)
pip install maturin
pip install maturin[patchelf]  # Linux/Windows å¹³å°
```

2. **æ„å»º Python åŒ…**

```bash
# Naive CUDA (åªèƒ½ç”¨äºå•å¡æ¨ç†) 
maturin build --release --features cuda,python

# Naive CUDA (+CUDA Graph, å®éªŒé˜¶æ®µ)
maturin build --release --features cuda,graph,python

# CUDA (æ”¯æŒContext-cacheä¸FP8 KV Cacheï¼Œä¸ä½¿ç”¨Flash attention) 
./build.sh --release --features cuda,nccl,python

# CUDA (+Flash attentionï¼Œä»…prefillæ—¶å¯ç”¨) 
./build.sh --release --features cuda,nccl,flash-attn,python

# CUDA (+Flash attentionï¼Œprefill/decodingå‡ä½¿ç”¨Flash attentionï¼Œç¼–è¯‘æ—¶é—´æœ€é•¿) 
./build.sh --release --features cuda,nccl,flash-context,python

# macOSï¼ˆMetal, æ”¯æŒContext-cacheä¸FP8 KV Cacheï¼Œä½†ä¸æ”¯æŒå¤šGPUæ¨ç†ï¼‰
maturin build --release --features metal,python

```

3. **å®‰è£…æ„å»ºå¥½çš„åŒ…ä¸ä¾èµ–**

```bash
pip install target/wheels/vllm_rs-*-cp38-abi3-*.whl --force-reinstall
pip install fastapi uvicorn
```


### âš™ï¸ å‘½ä»¤è¡Œå‚æ•°è¯´æ˜

| å‚æ•°          | æè¿°                                     |       |
| ----------- | -------------------------------------- | ----- |
| `--m`       | Hugginfaceæ¨¡å‹ID (ç”¨äºä¸‹è½½)               |    |
| `--w`       | Safetensoræ¨¡å‹è·¯å¾„           |       |
| `--f`       | å½“æŒ‡å®šModel IDæ—¶ä¸ºGGUFæ–‡ä»¶åï¼Œæˆ–æœªæŒ‡å®šæ—¶ä¸ºGGUFæœ¬åœ°æ–‡ä»¶è·¯å¾„                 |    |
| `--d`       | è®¾å¤‡ IDï¼Œä¾‹å¦‚ `--d 0`                       |       |
| `--max-num-seqs`   | åŒæ—¶å¤„ç†çš„æœ€å¤§è¯·æ±‚æ•°ï¼ˆé»˜è®¤ `32`, macOSå¹³å°ä¸º`8`ï¼‰   |       |
| `--max-tokens`     | å•æ¬¡æœ€å¤§è¾“å‡º token æ•°ï¼ˆé»˜è®¤ `4096`ï¼Œä¸Šé™ä¸ºæ¨¡å‹æ”¯æŒçš„æœ€å¤§é•¿åº¦ï¼‰ |       |
| `--batch`     | ä»…ç”¨äºæ€§èƒ½ (å¯ç”¨åä¼šå¿½ç•¥ `max-num-seqs` ä¸ `prompts`) |    |
| `--prompts` | è¾“å…¥çš„ promptï¼Œå¤šä¸ªä½¿ç”¨ \| åˆ†éš” |
| `--dtype`   | KV ç¼“å­˜æ•°æ®ç±»å‹ï¼š`bf16`ï¼ˆé»˜è®¤ï¼‰ã€`f16` æˆ– `f32`     |       |
| `--isq`   | å°†æœªé‡åŒ–æ¨¡å‹åŠ è½½ä¸ºGGUFé‡åŒ–æ¨¡å‹ï¼Œå¯é€‰`q2k`, `q4k`  ç­‰   |       |
| `--temperature`   | é‡‡æ ·æ¸©åº¦ (sampling temperature)ï¼Œæ§åˆ¶è¾“å‡ºâ€œéšæœºæ€§/åˆ›é€ æ€§â€çš„ä¸€ä¸ªè¶…å‚æ•°ï¼Œä»‹äº0-1ä¹‹é—´  |       |
| `--top-k`   | top-k æ§åˆ¶æ¨¡å‹åœ¨æ¯ä¸€æ­¥åªä»å‰ k ä¸ªæœ€é«˜æ¦‚ç‡çš„è¯é‡ŒæŒ‘é€‰ï¼Œk è¶Šå° â†’ è¶Šç¨³å®šï¼›k è¶Šå¤§ â†’ è¶Šéšæœº   |       |
| `--top-p`   | top-p é‡‡æ ·æ ¹æ®æ¦‚ç‡é˜ˆå€¼é€‰æ‹©åŠ¨æ€æ•°é‡çš„å€™é€‰ï¼ŒèŒƒå›´æ˜¯ [0,1]ï¼Œå¸¸ç”¨åœ¨ 0.8 ~ 0.95   |       |
| `--presence-penalty` | å‡ºç°æƒ©ç½šï¼Œæ§åˆ¶æ¨¡å‹æ˜¯å¦é¿å…å†æ¬¡æåŠ`å·²ç»å‡ºç°è¿‡çš„è¯`ã€‚<br> æ•°å€¼èŒƒå›´ [-2, 2]ï¼Œæ­£å€¼è¶Šå¤§ â†’ è¶Šå€¾å‘å¼•å…¥æ–°è¯æ±‡ï¼›è´Ÿå€¼ â†’ è¶Šå€¾å‘é‡å¤å·²å‡ºç°çš„è¯ | |
| `--frequency-penalty` | é¢‘ç‡æƒ©ç½šï¼Œæ§åˆ¶æ¨¡å‹æ˜¯å¦å‡å°‘`é«˜é¢‘é‡å¤è¯`çš„å‡ºç°ã€‚<br> æ•°å€¼èŒƒå›´ [-2, 2]ï¼Œæ­£å€¼è¶Šå¤§ â†’ é‡å¤æ¬¡æ•°è¶Šå¤šçš„è¯æƒ©ç½šè¶Šå¼ºï¼›è´Ÿå€¼ â†’ è¶Šé¼“åŠ±é‡å¤ä½¿ç”¨åŒä¸€è¯ | |
| `--server`       | æœåŠ¡æ¨¡å¼ï¼Œé€‚ç”¨äºRust CLIï¼ŒPythonä½¿ç”¨ `python -m vllm.server`        |       |
| `--fp8-kvcache`       | ä½¿ç”¨FP8 KV Cache (flash-contextæ²¡æœ‰å¯ç”¨æ—¶ç”Ÿæ•ˆ)                 |    |
| `--cpu-mem-fold`       | CPU KV Cacheå¤§å° (ä¸GPU KV Cacheçš„ç™¾åˆ†æ¯”ï¼Œé»˜è®¤ 1.0ï¼Œå–å€¼0.1 - 10.0)              |    |
| `--pd-server`       | ä½¿ç”¨PDåˆ†ç¦»æ¨¡å¼æ—¶ï¼ŒæŒ‡å®šå½“å‰å®ä¾‹ä¸ºPDæœåŠ¡å™¨ï¼ˆæ­¤æœåŠ¡å™¨ä»…ç”¨äºPrefillï¼‰            |    |
| `--pd-client`       | ä½¿ç”¨PDåˆ†ç¦»æ¨¡å¼æ—¶ï¼ŒæŒ‡å®šå½“å‰å®ä¾‹ä¸ºPDå®¢æˆ·ç«¯ï¼ˆæ­¤å®¢æˆ·ç«¯å°†é•¿çš„ä¸Šä¸‹æ–‡Prefillè¯·æ±‚å‘é€ç»™PDæœåŠ¡å™¨å¤„ç†ï¼‰|    |
| `--pd-url`       |  ä½¿ç”¨PDåˆ†ç¦»æ¨¡å¼æ—¶ï¼ŒPDæœåŠ¡å™¨å®ä¾‹å¦‚æŒ‡å®špd-urlï¼Œåˆ™é€šè¿‡TCP/IPé€šä¿¡ï¼ˆé€‚ç”¨äºPDæœåŠ¡å™¨ä¸å®¢æˆ·ç«¯åœ¨ä¸åŒæœåŠ¡å™¨ï¼‰ |    |

## ğŸ“Œ é¡¹ç›®çŠ¶æ€

> ğŸš§ **é¡¹ç›®ä»åœ¨ç§¯æå¼€å‘ä¸­ï¼Œæ¥å£ä¸åŠŸèƒ½å¯èƒ½å‘ç”Ÿå˜æ›´ã€‚**

## ğŸ› ï¸ å¼€å‘è®¡åˆ’ï¼ˆTODOï¼‰

* [x] Metal å¹³å°æ”¯æŒæ‰¹é‡æ¨ç†
* [x] æ”¯æŒ GGUF æ ¼å¼
* [x] CUDA å¹³å° Flash Attention æ”¯æŒ
* [x] CUDA Graph
* [x] OpenAI API å…¼å®¹æœåŠ¡å™¨ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
* [x] æŒç»­æ‰¹å¤„ç†
* [x] å¤šå¡å¹¶è¡Œæ¨ç†ï¼ˆSafetensorsæ¨¡å‹ã€GPTQ/AWQåŠGGUFé‡åŒ–æ¨¡å‹ï¼‰
* [x] Metal/macOSå¹³å°Promptå¤„ç†åŠ é€Ÿ
* [x] åˆ†å—é¢„å¡«å……ï¼ˆChunked Prefillï¼‰
* [x] ä¸Šä¸‹æ–‡ç¼“å­˜ (ä½¿ç”¨`context-cache`å‚æ•°)
* [x] ä»Hugginface Hubä¸‹è½½å¹¶åŠ è½½æ¨¡å‹
* [ ] ä»ModelScopeä¸‹è½½å¹¶åŠ è½½ (ä¸­å›½å¤§é™†åœ°åŒº)
* [x] Metal/macOSå¹³å°ä¸Šä¸‹æ–‡ç¼“å­˜
* [x] FP8 KV Cache (CUDA)
* [x] FP8 KV Cache (Metal)
* [ ] FP8 KV Cache (with Flash-Attn)
* [ ] æ”¯æŒæ›´å¤šæ¨¡å‹ç±»å‹ï¼ˆGLM 4.6, Kimi K2 Thinkingç­‰ï¼‰
* [x] CPU KV Cache å¸è½½
* [x] PDï¼ˆPrefill/Decodeï¼‰åˆ†ç¦»ï¼ˆCUDAï¼‰
* [x] PDï¼ˆPrefill/Decodeï¼‰åˆ†ç¦»ï¼ˆMetalï¼‰
* [ ] PD Client for Pythonï¼ˆPythonå…¨å±€é”é—®é¢˜ï¼‰

## ğŸ“š å‚è€ƒé¡¹ç›®

å‚è€ƒï¼š

* [Candle-vLLM](https://github.com/EricLBuehler/candle-vllm)
* Python nano-vllm é¡¹ç›®

---

ğŸ’¡ **å–œæ¬¢è¿™ä¸ªé¡¹ç›®ï¼Ÿæ¬¢è¿ â­ æ”¶è—å’Œå‚ä¸è´¡çŒ®ï¼**
