# ğŸš€ **vLLM.rs** â€“ ç”¨ Rust å®ç°çš„æç®€ vLLM

ä¸€ä¸ªæé€Ÿ âš¡ã€è½»é‡çš„ ğŸ¦€**Rust å®ç°ç‰ˆ vLLM**ã€‚

---

<p align="center">
  <a href="./ReadMe.md">English</a> |
  <a href="./ReadMe-CN.md">ç®€ä½“ä¸­æ–‡</a> |
</p>

## âœ¨ ä¸»è¦ç‰¹æ€§

* ğŸ”§ **çº¯ Rust åç«¯** â€“ å®Œå…¨**ä¸ä¾èµ– PyTorch**
* ğŸš€ **é«˜æ€§èƒ½** â€“ æ€§èƒ½ä¼˜äº vLLM å’Œ Nano-vLLM
* ğŸ§  **æç®€æ ¸å¿ƒ** â€“ æ ¸å¿ƒé€»è¾‘ä»… **< 1000 è¡Œ** Rust ä»£ç 
* ğŸ’» **è·¨å¹³å°æ”¯æŒ** â€“ æ”¯æŒ **CUDA**ï¼ˆLinux/Windowsï¼‰ä¸ **Metal**ï¼ˆmacOSï¼‰
* ğŸ¤– **å†…ç½®èŠå¤©/API æœåŠ¡** â€“ Rust åŸç”Ÿå®ç°çš„èŠå¤©ä¸ API æœåŠ¡
* ğŸ **è½»é‡ Python æ¥å£** â€“ ä½¿ç”¨ PyO3 æ„å»ºçš„ Python èŠå¤©æ¥å£
* ğŸ¤ **æ¬¢è¿è´¡çŒ®** â€“ æ¬¢è¿æäº¤ PRã€é—®é¢˜æˆ–ç»™é¡¹ç›®ç‚¹äº® â­ï¼

---

### æ€§èƒ½å¯¹æ¯”

> æ¨¡å‹: Qwen3-0.6B (BF16)ï¼›
> å¹¶å‘è¯·æ±‚æ•°: 256ï¼›
> Max Model Length: 1024ï¼›
> æ¯ä¸ªè¯·æ±‚æœ€å¤§è¾“å‡º: 1024

| æ¨ç†å¼•æ“ | è¾“å‡ºTokens | è€—æ—¶ (s) | ååç‡ (tokens/s) |
|------------------|---------------|----------|------------------------|
| vLLM (RTX 4070) (Reference)          | 133,966       | 98.37    | 1361.84                |
| Nano-vLLM (RTX 4070) (Reference)      | 133,966       | 93.41    | 1434.13                |
| **vLLM.rs** (**A100**)        | 257,792       | 25.21s    | **10216.44** (**æå‡30%+**)               |
| Nano-vLLM (A100)       | 262144       | 34.22s    |   7660.26      | 

#### å¤ç°æ­¥éª¤

**vLLM.rs**
```shell
# æœªå¯ç”¨Cuda Graphï¼Œæœªå¯ç”¨Flash Attentionï¼Œæ— æ¨¡å‹é¢„çƒ­ (æœ€ç»ˆæŠ¥å‘Š)
cargo run --release --features cuda -- --w /home/Qwen3-0.6B --batch 256 --max-tokens 1024 --max-model-len 1024
# æ—¥å¿—
2025-07-16T10:32:32.632729Z  INFO vllm_rs: --- Performance Metrics ---
2025-07-16T10:32:32.632764Z  INFO vllm_rs: â±ï¸ Prompt tokens: 4096 in 12.56s (326.17 tokens/s)
2025-07-16T10:32:32.632781Z  INFO vllm_rs: â±ï¸ Decoded tokens: 257792 in 25.21s (10216.44 tokens/s)

# å¯ç”¨cuda graph è·å¾—æ›´é«˜æ€§èƒ½
cargo run --release --features cuda,graph -- --w /home/Qwen3-0.6B --batch 256 --max-tokens 1024 --max-model-len 1024
# å¯ç”¨cuda graphå’Œflash attentionè·å¾—æœ€é«˜æ€§èƒ½ (ç¼–è¯‘flash attentionéœ€è¦è¾ƒé•¿æ—¶é—´)
cargo run --release --features cuda,flash-attn,graph -- --w /home/Qwen3-0.6B --batch 256 --max-tokens 1024 --max-model-len 1024 --flash
```

***Nano-vLLM** 

   ğŸ’¡ (ä¸ºå…¬å¹³æ¯”è¾ƒï¼Œè¯·ä¿®æ”¹æ‰€æœ‰è¯·æ±‚æœ€é•¿è¾“å‡ºä¸ºå›ºå®šå€¼ï¼ˆå¦‚1024ï¼‰ï¼Œè€Œééšæœºå€¼ï¼ˆ100-1024)ï¼‰
```shell
# é»˜è®¤å¯ç”¨cuda graphï¼Œå¯ç”¨flash attention ä¸æ¨¡å‹é¢„çƒ­
python3 bench.py
# æ—¥å¿—
Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.65s/it, Prefill=1tok/s, Decode=369tok/s]
Total: 262144tok, Time: 34.22s, Throughput: 7660.26tok/s
```

## ğŸ“¦ å®‰è£…ä¸ä½¿ç”¨

> âš ï¸ å¯ç”¨ Flash Attentionï¼ˆCUDAï¼‰æ—¶ï¼Œé¦–æ¬¡ç¼–è¯‘å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ã€‚

### ğŸ› ï¸ ç¯å¢ƒè¦æ±‚

* å®‰è£… [Rust å·¥å…·é“¾](https://www.rust-lang.org/tools/install)
* macOS å¹³å°éœ€å®‰è£… [Xcode å‘½ä»¤è¡Œå·¥å…·](https://mac.install.guide/commandlinetools/)
* æ„å»º Python æ¥å£éœ€å®‰è£… [Maturin](https://github.com/PyO3/maturin)

---

## ğŸ å¿«é€Ÿ Python ç¤ºä¾‹
   ğŸ’¡ ç¼–è¯‘vllm.rs PythonåŒ…å¯å‚è§`API æœåŠ¡æ¨¡å¼ï¼ˆPython æ¥å£ï¼‰`
```python
from vllm_rs import Engine, EngineConfig, SamplingParams, Message
cfg = EngineConfig(model_path="/path/Qwen3-8B-Q2_K.gguf", max_model_len=4096)
engine = Engine(cfg, "bf16")
params = SamplingParams(temperature=0.6, max_tokens=256)
prompt = engine.apply_chat_template([Message("user", "How are you?")], True)

# åŒæ­¥æ‰¹é‡ç”Ÿæˆ
outputs = engine.generate_sync([params,params], [prompt, prompt])
print(outputs)

# å•è¯·æ±‚æµå¼ç”Ÿæˆ
stream = engine.generate_stream(params, prompt)
for token in stream:
    print(token)
```

---

## ğŸ¤–âœ¨ äº¤äº’æ¨¡å¼ï¼ˆçº¯ Rust CLIï¼‰

ä½¿ç”¨ `--i` å¯ç”¨äº¤äº’æ¨¡å¼ï¼Œ`--w` æŒ‡å®šæ¨¡å‹è·¯å¾„ï¼š

```bash
# CUDAï¼ˆçŸ­ä¸Šä¸‹æ–‡ï¼‰
cargo run --release --features cuda -- --i --w /path/qwq-32b-q4_k_m.gguf

# CUDA + Flash Attentionï¼ˆè¶…é•¿ä¸Šä¸‹æ–‡ï¼Œå¦‚ 32k tokensï¼‰
cargo run --release --features cuda,flash-attn -- --i --w /path/qwq-32b-q4_k_m.gguf

# macOSï¼ˆMetalï¼‰
cargo run --release --features metal -- --i --w /path/DeepSeek-R1-Distill-Llama-8B-Q2_K.gguf
```

---

## ğŸŒâœ¨ API æœåŠ¡æ¨¡å¼ï¼ˆPython æ¥å£ï¼‰

1. **å®‰è£… Maturin**

```bash
pip install maturin
pip install maturin[patchelf]  # Linux/Windows å¹³å°
```

2. **æ„å»º Python åŒ…**

```bash
# CUDAï¼ˆè¾ƒçŸ­ä¸Šä¸‹æ–‡ï¼‰
maturin build --release --features cuda,python

# CUDA + Flash Attention (è¶…é•¿ä¸Šä¸‹æ–‡ (>32kæ—¶) æ¨èå¯ç”¨ï¼‰
maturin build --release --features cuda,flash-attn,python

# macOSï¼ˆMetalï¼‰
maturin build --release --features metal,python
```

3. **å®‰è£…æ„å»ºå¥½çš„åŒ…ä¸ä¾èµ–**

```bash
pip install target/wheels/vllm_rs-0.1.0-cp38-abi3-*.whl
pip install fastapi uvicorn
```

4. **å¯åŠ¨ OpenAI API æœåŠ¡**
   ğŸ’¡ä½ å¯ä»¥ä½¿ç”¨**ä»»ä½•å…¼å®¹ OpenAI API çš„å®¢æˆ·ç«¯**è¿›è¡Œäº¤äº’ã€‚
```bash
# å¯åŠ¨ OpenAI å…¼å®¹çš„ API æœåŠ¡ï¼ˆç›‘å¬ http://0.0.0.0:8000ï¼‰
# openai.base_url = "http://localhost:2000/v1/"
# openai.api_key = "EMPTY"
# æ·»åŠ `--flash`é€‰é¡¹ä»¥å¯ç”¨Flash attention ï¼ˆmaturinç”ŸæˆwhlåŒ…æ—¶éœ€æ·»åŠ `flash-attn`ï¼‰
python example/server.py --w /path/qwq-32b-q4_k_m.gguf --host 0.0.0.0 --port 8000
```


---

### å…¶ä»– Python ç¤ºä¾‹

```bash
# äº¤äº’å¼èŠå¤©
python3 example/chat.py --i --w /path/qwq-32b-q4_k_m.gguf

# æ‰¹é‡åŒæ­¥ç¤ºä¾‹
python3 example/completion.py --w /path/qwq-32b-q4_k_m.gguf --prompts "How are you? | How to make money?"
```

---

### ğŸ“½ï¸ æ¼”ç¤ºè§†é¢‘

ğŸ‰ è§‚çœ‹é¡¹ç›®è¿è¡Œæ¼”ç¤ºï¼š

<video src="https://github.com/user-attachments/assets/0751471b-a0c4-45d7-acc6-99a3e91e4c91" width="70%"></video>

---

## ğŸ§¾ è¡¥å…¨æ¨¡å¼ï¼ˆRust CLIï¼‰

### GGUF æ¨¡å‹

```bash
# CUDA
cargo run --release --features cuda -- --w /path/qwq-32b-q4_k_m.gguf --prompts "How are you today?"

# CUDA + Flash Attention
cargo run --release --features cuda,flash-attn -- --w /path/qwq-32b-q4_k_m.gguf --prompts "How are you today?"

# Metalï¼ˆmacOSï¼‰
cargo run --release --features metal -- --w /path/qwq-32b-q4_k_m.gguf --prompts "How are you today?"
```

### Python è°ƒç”¨ï¼š

```bash
python example/completion.py --w /path/qwq-32b-q4_k_m.gguf --prompts "How are you? | How to make money?"
```

### Safetensor æ¨¡å‹ï¼ˆæœªé‡åŒ–ï¼‰

```bash
# CUDA
cargo run --release --features cuda,flash-attn -- --w /path/Qwen3-8B/ --prompts "How are you today?"

# Metalï¼ˆmacOSï¼‰
cargo run --release --features metal -- --w /path/Qwen3-8B/ --prompts "How are you today?"
```

---

## ğŸ“š æ‰¹é‡è¯·æ±‚æ”¯æŒ

å¤šä¸ª prompt ä½¿ç”¨ `|` åˆ†éš”ï¼š

```bash
# GGUF æ¨¡å‹ï¼ˆRustï¼‰
cargo run --release --features cuda,flash-attn -- --w /path/qwq-32b-q4_k_m.gguf --prompts "Talk about China. | Talk about America." --max-model-len 1024

# Safetensor æ¨¡å‹ï¼ˆRustï¼‰
cargo run --release --features metal -- --w /path/Qwen3-8B/ --prompts "Talk about China. | Talk about America."

# GGUF æ¨¡å‹ï¼ˆPythonï¼‰
python3 example/completion.py --w /path/qwq-32b-q4_k_m.gguf --prompts "How are you? | How to make money?" --max-model-len 1024
```

---

## ğŸ—œï¸ å®æ—¶é‡åŒ–ï¼ˆGGUF æ ¼å¼è½¬æ¢ï¼‰

é‡åŒ–è¿‡ç¨‹å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼š

```bash
# macOS
cargo run --release --features metal -- --w /path/Qwen3-0.6B/ --quant q4k --prompts "How are you today?"

# CUDA
cargo run --release --features cuda,flash-attn -- --w /path/Qwen3-8B/ --quant q4k --prompts "How are you today?"
```

---

## ğŸ“„ ç¤ºä¾‹è¾“å‡º

**å•æ¡è¯·æ±‚**ï¼ˆQwen3-0.6Bï¼ŒBF16ï¼ŒmacOS Metalï¼‰ï¼š

```bash
cargo run --features metal -- --w /path/Qwen3-0.6B/ --prompts "How are you today?"
```

```
<think>
ç”¨æˆ·æé—®ï¼š"How are you today?"...
</think>

ä½ å¥½å‘€ï¼ä»Šå¤©æ„Ÿè§‰æ€ä¹ˆæ ·ï¼Ÿæˆ‘åœ¨è¿™é‡Œå¯ä»¥å¸®ä½ è§£ç­”ä»»ä½•é—®é¢˜ï¼ğŸ˜Š æœ‰éœ€è¦å°½ç®¡å‘Šè¯‰æˆ‘ï¼
```

---


## âš™ï¸ å‘½ä»¤è¡Œå‚æ•°è¯´æ˜

| å‚æ•°          | æè¿°                                     |       |
| ----------- | -------------------------------------- | ----- |
| `--w`       | æ¨¡å‹è·¯å¾„ï¼ˆSafetensor ç›®å½•æˆ– GGUF æ–‡ä»¶ï¼‰           |       |
| `--d`       | è®¾å¤‡ IDï¼Œä¾‹å¦‚ `--d 0`                       |       |
| `--max_num_seqs`   | åŒæ—¶å¤„ç†çš„æœ€å¤§è¯·æ±‚æ•°ï¼ˆé»˜è®¤ `32`ï¼‰               |       |
| `--max_tokens`     | å•æ¬¡æœ€å¤§è¾“å‡º token æ•°ï¼ˆé»˜è®¤ `4096`ï¼Œä¸Šé™ä¸ºæ¨¡å‹æ”¯æŒçš„æœ€å¤§é•¿åº¦ï¼‰ |       |
| `--batch`     | ä»…ç”¨äºæ€§èƒ½ (å¯ç”¨åä¼šå¿½ç•¥ `max-num-seqs` ä¸ `prompts`) |    |
| `--prompts` | è¾“å…¥çš„ promptï¼Œå¤šä¸ªä½¿ç”¨ \`                     | \` åˆ†éš” |
| `--dtype`   | KV ç¼“å­˜æ•°æ®ç±»å‹ï¼š`bf16`ï¼ˆé»˜è®¤ï¼‰ã€`f16` æˆ– `f32`     |       |

---

## ğŸ§  æ”¯æŒçš„æ¨¡å‹æ¶æ„

* âœ… LLaMa ç³»åˆ—ï¼ˆLLaMa2ã€LLaMa3ï¼‰
* âœ… Qwen ç³»åˆ—ï¼ˆQwen2ã€Qwen3ï¼‰
* âœ… Mistral

æ”¯æŒ **Safetensor** å’Œ **GGUF** æ ¼å¼ã€‚

---

## ğŸ“Œ é¡¹ç›®çŠ¶æ€

> ğŸš§ **é¡¹ç›®ä»åœ¨ç§¯æå¼€å‘ä¸­ï¼Œæ¥å£ä¸åŠŸèƒ½å¯èƒ½å‘ç”Ÿå˜æ›´ã€‚**

---

## ğŸ› ï¸ å¼€å‘è®¡åˆ’ï¼ˆTODOï¼‰

* [x] Metal å¹³å°æ”¯æŒæ‰¹é‡æ¨ç†
* [x] æ”¯æŒ GGUF æ ¼å¼
* [x] CUDA å¹³å° Flash Attention æ”¯æŒ
* [x] OpenAI API å…¼å®¹æœåŠ¡å™¨ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
* [x] æŒç»­æ‰¹å¤„ç†
* [ ] å¤šå¡å¹¶è¡Œæ¨ç†
* [ ] æ”¯æŒæ›´å¤šæ¨¡å‹ç±»å‹

---

## ğŸ“š å‚è€ƒé¡¹ç›®

æ ¸å¿ƒæ€è·¯å‚è€ƒï¼š

* [Candle-vLLM](https://github.com/EricLBuehler/candle-vllm)
* Python nano-vllm é¡¹ç›®

---

ğŸ’¡ **å–œæ¬¢è¿™ä¸ªé¡¹ç›®ï¼Ÿæ¬¢è¿ â­ æ”¶è—å’Œå‚ä¸è´¡çŒ®ï¼**
